{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Many ML problems involve thousands or even millions \n",
    "    of features for each training instance. Not only does this make\n",
    "    training extremely slow, it can also make it much harder to find\n",
    "    a good solution!\n",
    "    \n",
    "    Fortunately in real-world problems, it is often possible to reduce\n",
    "    the number of features considerably, turning an intractanble \n",
    "    problem into a tracable one. For instance, the images of MNIST dataset\n",
    "    always have the border pixels white => we could completely drop them\n",
    "    from the training set, without losing much information. \n",
    "    Moreover, two neighbouring pixels are often highly correlated => \n",
    "    we can merge them into a single pixel, by taking the mean of the \n",
    "    two pixel intensities, w/o losing much info!!\n",
    "    \n",
    "    Apart from speeding up training, dimensionality reduction is also\n",
    "    extremely useful for data visualization - aka dataViz. Reducing\n",
    "    the nr of dims down to 2 or 3 make it possible to plot\n",
    "    a high dim training set on a graph and often gain important insights \n",
    "    by visually detecting patterns, like clusters. \n",
    "    \n",
    "    Two main approaches to DR are: \n",
    "         - projection\n",
    "         - Manifold learning\n",
    "\n",
    "    others include PCA, Kernel PCA, and LLE\n",
    "   \n",
    "   It turns out that many things behave very differently in high-dimensional space. For example, if you pick a\n",
    "    random point in a unit square (a 1 × 1 square), it will have only about a 0.4% chance of being located less\n",
    "    than 0.001 from a border (in other words, it is very unlikely that a random point will be “extreme” along\n",
    "    any dimension). But in a 10,000-dimensional unit hypercube (a 1 × 1 × × 1 cube, with ten thousand 1s),\n",
    "    this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close\n",
    "    to the border.It turns out that many things behave very differently in high-dimensional space. For example, if you pick a\n",
    "    random point in a unit square (a 1 × 1 square), it will have only about a 0.4% chance of being located less\n",
    "    than 0.001 from a border (in other words, it is very unlikely that a random point will be “extreme” along\n",
    "    any dimension). But in a 10,000-dimensional unit hypercube (a 1 × 1 × × 1 cube, with ten thousand 1s),\n",
    "    this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close\n",
    "    to the border.\n",
    "    \n",
    "    Projection:\n",
    "    In most problems, training instances are not spread out uniformly\n",
    "    across all dimensions. Many features are almost constant, while\n",
    "    others are highly correlated => all training instances actually\n",
    "    lie within a much lower-dimensional subspace of the high-dimensional\n",
    "    space.\n",
    "    \n",
    "    For instance, we project EVERY TRAINING INSTANCE PERPENDICULAR TO A NEW \n",
    "    2D SUBSET, hence reduce from 3D to 2D;\n",
    "    \n",
    "    PCA first identifies the hyperplane that lies closest to the data, \n",
    "    and then it projects the data onto it. \n",
    "    \n",
    "    PCA identifies the axis that accounts for the largest amount\n",
    "    of variance in the training set. \n",
    "    It also finds a second axis, orthogonal to the first one, that \n",
    "    accounts for the largest amount\n",
    "    of remaining variance. \n",
    "    \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "X_a = np.c_[np.ones((100,1)),X]\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X_a)\n",
    "\n",
    "X2D\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
