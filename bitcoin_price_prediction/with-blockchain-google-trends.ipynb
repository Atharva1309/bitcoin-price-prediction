{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Imports](#Imports)\n",
    "2. [Load Data](#load_data)\n",
    "4. [ML Pipeline](#ml_pipeline)\n",
    "   - [Split Data (Testing, Training Data Sets)](#split_data)\n",
    "   - [Set Window Length](#win_len_metrics)\n",
    "   - [Fill Training, Test Data](#fill_training_test)\n",
    "   - [Load Model](#load_model)\n",
    "   - [Train Model](#train_model)\n",
    "   - [Graph Predicted Values with Training Set](#graph_pred_training_set)\n",
    "   - [Graph Predicted Values with Test Data](#graph_pred_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"imports\"></a>Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from_date=\"2013-04-28\"\n",
    "to_date=\"2018-10-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <a name=\"load_data\"></a>Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>btc_high</th>\n",
       "      <th>btc_close</th>\n",
       "      <th>btc_volume</th>\n",
       "      <th>btc_market_cap</th>\n",
       "      <th>bch_avg_block_size</th>\n",
       "      <th>bch_transactions</th>\n",
       "      <th>bch_mining_revenue</th>\n",
       "      <th>bch_accounts</th>\n",
       "      <th>sp_close</th>\n",
       "      <th>dj_close</th>\n",
       "      <th>google_trends_bitcoin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.055634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.158024</td>\n",
       "      <td>0.001304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.055634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.158024</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.138178</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>0.011759</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.158024</td>\n",
       "      <td>0.003644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.138178</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.166638</td>\n",
       "      <td>0.133303</td>\n",
       "      <td>0.004112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.319659</td>\n",
       "      <td>0.210326</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.170314</td>\n",
       "      <td>0.134174</td>\n",
       "      <td>0.002708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.319659</td>\n",
       "      <td>0.210326</td>\n",
       "      <td>0.017024</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.146305</td>\n",
       "      <td>0.111596</td>\n",
       "      <td>0.004112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.361094</td>\n",
       "      <td>0.180002</td>\n",
       "      <td>0.011350</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.103488</td>\n",
       "      <td>0.076460</td>\n",
       "      <td>0.005516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.361094</td>\n",
       "      <td>0.180002</td>\n",
       "      <td>0.014625</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.084372</td>\n",
       "      <td>0.061449</td>\n",
       "      <td>0.005516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>0.257161</td>\n",
       "      <td>0.181665</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.084372</td>\n",
       "      <td>0.061449</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.257161</td>\n",
       "      <td>0.181665</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.084372</td>\n",
       "      <td>0.061449</td>\n",
       "      <td>0.003644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.305948</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.085861</td>\n",
       "      <td>0.066115</td>\n",
       "      <td>0.004112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.305948</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.099485</td>\n",
       "      <td>0.076650</td>\n",
       "      <td>0.003644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.337240</td>\n",
       "      <td>0.225642</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.043985</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.337240</td>\n",
       "      <td>0.225642</td>\n",
       "      <td>0.014796</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.084199</td>\n",
       "      <td>0.064368</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511714</td>\n",
       "      <td>0.203998</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.011540</td>\n",
       "      <td>0.046520</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>0.009729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.511714</td>\n",
       "      <td>0.203998</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.011540</td>\n",
       "      <td>0.046520</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>0.010197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.460682</td>\n",
       "      <td>0.229301</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.011540</td>\n",
       "      <td>0.046520</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>0.006452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.460682</td>\n",
       "      <td>0.229301</td>\n",
       "      <td>0.008962</td>\n",
       "      <td>0.011540</td>\n",
       "      <td>0.046520</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>0.007388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.469945</td>\n",
       "      <td>0.272449</td>\n",
       "      <td>0.015609</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.047428</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.005984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.469945</td>\n",
       "      <td>0.272449</td>\n",
       "      <td>0.010360</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.027458</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.007388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>2016-01-21</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.599769</td>\n",
       "      <td>0.287397</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.036227</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>0.005984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.599769</td>\n",
       "      <td>0.287397</td>\n",
       "      <td>0.009578</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.070638</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.005048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>2016-01-23</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.260769</td>\n",
       "      <td>0.256449</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>0.070638</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.260769</td>\n",
       "      <td>0.256449</td>\n",
       "      <td>0.016646</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>0.070638</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.322734</td>\n",
       "      <td>0.288564</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>0.043570</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.322734</td>\n",
       "      <td>0.288564</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>0.067670</td>\n",
       "      <td>0.045401</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.434422</td>\n",
       "      <td>0.340966</td>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.048898</td>\n",
       "      <td>0.025454</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.434422</td>\n",
       "      <td>0.340966</td>\n",
       "      <td>0.012492</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.058348</td>\n",
       "      <td>0.036663</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.340409</td>\n",
       "      <td>0.328480</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.100901</td>\n",
       "      <td>0.072180</td>\n",
       "      <td>0.002708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2016-01-30</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.340409</td>\n",
       "      <td>0.328480</td>\n",
       "      <td>0.014081</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.100901</td>\n",
       "      <td>0.072180</td>\n",
       "      <td>0.001304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>0.318471</td>\n",
       "      <td>0.325368</td>\n",
       "      <td>0.166823</td>\n",
       "      <td>0.338010</td>\n",
       "      <td>0.623792</td>\n",
       "      <td>0.395497</td>\n",
       "      <td>0.241866</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.994409</td>\n",
       "      <td>0.984135</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>0.316368</td>\n",
       "      <td>0.323616</td>\n",
       "      <td>0.165911</td>\n",
       "      <td>0.336238</td>\n",
       "      <td>0.623792</td>\n",
       "      <td>0.395497</td>\n",
       "      <td>0.234711</td>\n",
       "      <td>0.967425</td>\n",
       "      <td>0.993355</td>\n",
       "      <td>0.995124</td>\n",
       "      <td>0.038824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>0.314319</td>\n",
       "      <td>0.320819</td>\n",
       "      <td>0.162050</td>\n",
       "      <td>0.333391</td>\n",
       "      <td>0.596123</td>\n",
       "      <td>0.393782</td>\n",
       "      <td>0.201185</td>\n",
       "      <td>0.968374</td>\n",
       "      <td>0.995244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-10-04</td>\n",
       "      <td>0.315935</td>\n",
       "      <td>0.324692</td>\n",
       "      <td>0.159996</td>\n",
       "      <td>0.337423</td>\n",
       "      <td>0.596123</td>\n",
       "      <td>0.393782</td>\n",
       "      <td>0.209951</td>\n",
       "      <td>0.969132</td>\n",
       "      <td>0.973549</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.038824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>0.316965</td>\n",
       "      <td>0.327086</td>\n",
       "      <td>0.152987</td>\n",
       "      <td>0.339926</td>\n",
       "      <td>0.779142</td>\n",
       "      <td>0.401798</td>\n",
       "      <td>0.220729</td>\n",
       "      <td>0.970550</td>\n",
       "      <td>0.958990</td>\n",
       "      <td>0.965855</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-10-06</td>\n",
       "      <td>0.317215</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.135695</td>\n",
       "      <td>0.338115</td>\n",
       "      <td>0.779142</td>\n",
       "      <td>0.401798</td>\n",
       "      <td>0.189843</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.958990</td>\n",
       "      <td>0.965855</td>\n",
       "      <td>0.032496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>0.317872</td>\n",
       "      <td>0.326065</td>\n",
       "      <td>0.137664</td>\n",
       "      <td>0.338942</td>\n",
       "      <td>0.586844</td>\n",
       "      <td>0.318888</td>\n",
       "      <td>0.181914</td>\n",
       "      <td>0.972805</td>\n",
       "      <td>0.958990</td>\n",
       "      <td>0.965855</td>\n",
       "      <td>0.027750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-10-08</td>\n",
       "      <td>0.319575</td>\n",
       "      <td>0.328640</td>\n",
       "      <td>0.165920</td>\n",
       "      <td>0.341633</td>\n",
       "      <td>0.586844</td>\n",
       "      <td>0.318888</td>\n",
       "      <td>0.207299</td>\n",
       "      <td>0.974315</td>\n",
       "      <td>0.957955</td>\n",
       "      <td>0.969412</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-10-09</td>\n",
       "      <td>0.318882</td>\n",
       "      <td>0.328139</td>\n",
       "      <td>0.149179</td>\n",
       "      <td>0.341150</td>\n",
       "      <td>0.783982</td>\n",
       "      <td>0.372411</td>\n",
       "      <td>0.198751</td>\n",
       "      <td>0.975159</td>\n",
       "      <td>0.954242</td>\n",
       "      <td>0.964379</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-10-10</td>\n",
       "      <td>0.317811</td>\n",
       "      <td>0.325154</td>\n",
       "      <td>0.157865</td>\n",
       "      <td>0.338107</td>\n",
       "      <td>0.783982</td>\n",
       "      <td>0.372411</td>\n",
       "      <td>0.190925</td>\n",
       "      <td>0.975971</td>\n",
       "      <td>0.868318</td>\n",
       "      <td>0.889897</td>\n",
       "      <td>0.041988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>0.315095</td>\n",
       "      <td>0.307944</td>\n",
       "      <td>0.216405</td>\n",
       "      <td>0.320383</td>\n",
       "      <td>0.717173</td>\n",
       "      <td>0.460686</td>\n",
       "      <td>0.210910</td>\n",
       "      <td>0.977460</td>\n",
       "      <td>0.816297</td>\n",
       "      <td>0.841017</td>\n",
       "      <td>0.056227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>0.301995</td>\n",
       "      <td>0.308902</td>\n",
       "      <td>0.157690</td>\n",
       "      <td>0.321405</td>\n",
       "      <td>0.717173</td>\n",
       "      <td>0.460686</td>\n",
       "      <td>0.222729</td>\n",
       "      <td>0.978381</td>\n",
       "      <td>0.851480</td>\n",
       "      <td>0.866729</td>\n",
       "      <td>0.043570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>0.300981</td>\n",
       "      <td>0.309499</td>\n",
       "      <td>0.127476</td>\n",
       "      <td>0.322055</td>\n",
       "      <td>0.630997</td>\n",
       "      <td>0.270332</td>\n",
       "      <td>0.182795</td>\n",
       "      <td>0.979765</td>\n",
       "      <td>0.851480</td>\n",
       "      <td>0.866729</td>\n",
       "      <td>0.034078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-10-14</td>\n",
       "      <td>0.303756</td>\n",
       "      <td>0.309757</td>\n",
       "      <td>0.128370</td>\n",
       "      <td>0.322349</td>\n",
       "      <td>0.630997</td>\n",
       "      <td>0.270332</td>\n",
       "      <td>0.195646</td>\n",
       "      <td>0.980471</td>\n",
       "      <td>0.851480</td>\n",
       "      <td>0.866729</td>\n",
       "      <td>0.034078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-10-15</td>\n",
       "      <td>0.334285</td>\n",
       "      <td>0.325730</td>\n",
       "      <td>0.308338</td>\n",
       "      <td>0.338883</td>\n",
       "      <td>0.750784</td>\n",
       "      <td>0.475545</td>\n",
       "      <td>0.156212</td>\n",
       "      <td>0.981232</td>\n",
       "      <td>0.836648</td>\n",
       "      <td>0.858720</td>\n",
       "      <td>0.048317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-10-16</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.325707</td>\n",
       "      <td>0.169924</td>\n",
       "      <td>0.338894</td>\n",
       "      <td>0.750784</td>\n",
       "      <td>0.475545</td>\n",
       "      <td>0.235753</td>\n",
       "      <td>0.982717</td>\n",
       "      <td>0.890321</td>\n",
       "      <td>0.907777</td>\n",
       "      <td>0.040406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>0.315829</td>\n",
       "      <td>0.323006</td>\n",
       "      <td>0.170496</td>\n",
       "      <td>0.336139</td>\n",
       "      <td>0.905316</td>\n",
       "      <td>0.453702</td>\n",
       "      <td>0.198354</td>\n",
       "      <td>0.983912</td>\n",
       "      <td>0.889677</td>\n",
       "      <td>0.899562</td>\n",
       "      <td>0.038824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-10-18</td>\n",
       "      <td>0.314121</td>\n",
       "      <td>0.319467</td>\n",
       "      <td>0.163594</td>\n",
       "      <td>0.332519</td>\n",
       "      <td>0.905316</td>\n",
       "      <td>0.453702</td>\n",
       "      <td>0.203013</td>\n",
       "      <td>0.984982</td>\n",
       "      <td>0.852978</td>\n",
       "      <td>0.870262</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>0.310374</td>\n",
       "      <td>0.318876</td>\n",
       "      <td>0.149097</td>\n",
       "      <td>0.331945</td>\n",
       "      <td>0.859798</td>\n",
       "      <td>0.432016</td>\n",
       "      <td>0.201972</td>\n",
       "      <td>0.986300</td>\n",
       "      <td>0.852070</td>\n",
       "      <td>0.876072</td>\n",
       "      <td>0.038824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-10-20</td>\n",
       "      <td>0.310579</td>\n",
       "      <td>0.320119</td>\n",
       "      <td>0.140709</td>\n",
       "      <td>0.333265</td>\n",
       "      <td>0.859798</td>\n",
       "      <td>0.432016</td>\n",
       "      <td>0.205438</td>\n",
       "      <td>0.987439</td>\n",
       "      <td>0.852070</td>\n",
       "      <td>0.876072</td>\n",
       "      <td>0.029332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>0.313555</td>\n",
       "      <td>0.319762</td>\n",
       "      <td>0.135438</td>\n",
       "      <td>0.332930</td>\n",
       "      <td>0.574602</td>\n",
       "      <td>0.291594</td>\n",
       "      <td>0.206622</td>\n",
       "      <td>0.988874</td>\n",
       "      <td>0.852070</td>\n",
       "      <td>0.876072</td>\n",
       "      <td>0.030914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-10-22</td>\n",
       "      <td>0.312916</td>\n",
       "      <td>0.320013</td>\n",
       "      <td>0.153044</td>\n",
       "      <td>0.333229</td>\n",
       "      <td>0.574602</td>\n",
       "      <td>0.291594</td>\n",
       "      <td>0.200059</td>\n",
       "      <td>0.990106</td>\n",
       "      <td>0.841268</td>\n",
       "      <td>0.864707</td>\n",
       "      <td>0.035660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.319416</td>\n",
       "      <td>0.154862</td>\n",
       "      <td>0.332652</td>\n",
       "      <td>0.733930</td>\n",
       "      <td>0.430811</td>\n",
       "      <td>0.225885</td>\n",
       "      <td>0.991321</td>\n",
       "      <td>0.827480</td>\n",
       "      <td>0.853427</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-10-24</td>\n",
       "      <td>0.311810</td>\n",
       "      <td>0.320467</td>\n",
       "      <td>0.142621</td>\n",
       "      <td>0.333766</td>\n",
       "      <td>0.733930</td>\n",
       "      <td>0.430811</td>\n",
       "      <td>0.223685</td>\n",
       "      <td>0.992511</td>\n",
       "      <td>0.750697</td>\n",
       "      <td>0.798986</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>0.310931</td>\n",
       "      <td>0.319445</td>\n",
       "      <td>0.134469</td>\n",
       "      <td>0.332745</td>\n",
       "      <td>0.885125</td>\n",
       "      <td>0.434580</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.993774</td>\n",
       "      <td>0.795601</td>\n",
       "      <td>0.834903</td>\n",
       "      <td>0.035660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>0.310608</td>\n",
       "      <td>0.319364</td>\n",
       "      <td>0.137640</td>\n",
       "      <td>0.332697</td>\n",
       "      <td>0.885125</td>\n",
       "      <td>0.434580</td>\n",
       "      <td>0.197362</td>\n",
       "      <td>0.995012</td>\n",
       "      <td>0.753048</td>\n",
       "      <td>0.808378</td>\n",
       "      <td>0.035660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-27</td>\n",
       "      <td>0.311071</td>\n",
       "      <td>0.319659</td>\n",
       "      <td>0.141302</td>\n",
       "      <td>0.333036</td>\n",
       "      <td>0.706986</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.206547</td>\n",
       "      <td>0.996388</td>\n",
       "      <td>0.753048</td>\n",
       "      <td>0.808378</td>\n",
       "      <td>0.030914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>0.310810</td>\n",
       "      <td>0.319973</td>\n",
       "      <td>0.143483</td>\n",
       "      <td>0.333395</td>\n",
       "      <td>0.706986</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.194865</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.753048</td>\n",
       "      <td>0.808378</td>\n",
       "      <td>0.027750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-29</td>\n",
       "      <td>0.310877</td>\n",
       "      <td>0.311936</td>\n",
       "      <td>0.175178</td>\n",
       "      <td>0.325123</td>\n",
       "      <td>0.809491</td>\n",
       "      <td>0.484865</td>\n",
       "      <td>0.200871</td>\n",
       "      <td>0.998593</td>\n",
       "      <td>0.737217</td>\n",
       "      <td>0.786405</td>\n",
       "      <td>0.038824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>0.303846</td>\n",
       "      <td>0.312022</td>\n",
       "      <td>0.157590</td>\n",
       "      <td>0.325249</td>\n",
       "      <td>0.809491</td>\n",
       "      <td>0.484865</td>\n",
       "      <td>0.208186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774778</td>\n",
       "      <td>0.825062</td>\n",
       "      <td>0.037242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1034 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  btc_high  btc_close  btc_volume  btc_market_cap  \\\n",
       "1033 2016-01-01  0.003109   0.003659    0.000326        0.003217   \n",
       "1032 2016-01-02  0.003100   0.003612    0.000066        0.003180   \n",
       "1031 2016-01-03  0.002982   0.003433    0.000467        0.003024   \n",
       "1030 2016-01-04  0.003022   0.003594    0.000418        0.003175   \n",
       "1029 2016-01-05  0.003004   0.003535    0.000252        0.003127   \n",
       "1028 2016-01-06  0.002887   0.003386    0.000232        0.002998   \n",
       "1027 2016-01-07  0.004252   0.004898    0.002480        0.004360   \n",
       "1026 2016-01-08  0.004463   0.004646    0.001196        0.004140   \n",
       "1025 2016-01-09  0.004042   0.004353    0.000158        0.003882   \n",
       "1024 2016-01-10  0.003721   0.004373    0.000314        0.003906   \n",
       "1023 2016-01-11  0.003840   0.004396    0.000501        0.003933   \n",
       "1022 2016-01-12  0.003715   0.003730    0.003657        0.003340   \n",
       "1021 2016-01-13  0.003056   0.003556    0.006105        0.003189   \n",
       "1020 2016-01-14  0.002961   0.003448    0.000648        0.003097   \n",
       "1019 2016-01-15  0.002806   0.000000    0.005243        0.000000   \n",
       "1018 2016-01-16  0.000792   0.001213    0.003857        0.001095   \n",
       "1017 2016-01-17  0.000812   0.000939    0.000706        0.000853   \n",
       "1016 2016-01-18  0.000667   0.001194    0.001087        0.001088   \n",
       "1015 2016-01-19  0.000648   0.000827    0.000769        0.000761   \n",
       "1014 2016-01-20  0.002552   0.002922    0.003914        0.002652   \n",
       "1013 2016-01-21  0.002431   0.002401    0.001672        0.002187   \n",
       "1012 2016-01-22  0.001799   0.000949    0.002647        0.000885   \n",
       "1011 2016-01-23  0.000994   0.001210    0.001165        0.001126   \n",
       "1010 2016-01-24  0.001549   0.002020    0.001105        0.001860   \n",
       "1009 2016-01-25  0.001388   0.001432    0.001283        0.001336   \n",
       "1008 2016-01-26  0.001158   0.001454    0.001244        0.001361   \n",
       "1007 2016-01-27  0.001110   0.001601    0.000794        0.001498   \n",
       "1006 2016-01-28  0.001042   0.000834    0.001291        0.000810   \n",
       "1005 2016-01-29  0.000478   0.000791    0.002419        0.000777   \n",
       "1004 2016-01-30  0.000303   0.000728    0.000074        0.000725   \n",
       "...         ...       ...        ...         ...             ...   \n",
       "29   2018-10-01  0.318471   0.325368    0.166823        0.338010   \n",
       "28   2018-10-02  0.316368   0.323616    0.165911        0.336238   \n",
       "27   2018-10-03  0.314319   0.320819    0.162050        0.333391   \n",
       "26   2018-10-04  0.315935   0.324692    0.159996        0.337423   \n",
       "25   2018-10-05  0.316965   0.327086    0.152987        0.339926   \n",
       "24   2018-10-06  0.317215   0.325300    0.135695        0.338115   \n",
       "23   2018-10-07  0.317872   0.326065    0.137664        0.338942   \n",
       "22   2018-10-08  0.319575   0.328640    0.165920        0.341633   \n",
       "21   2018-10-09  0.318882   0.328139    0.149179        0.341150   \n",
       "20   2018-10-10  0.317811   0.325154    0.157865        0.338107   \n",
       "19   2018-10-11  0.315095   0.307944    0.216405        0.320383   \n",
       "18   2018-10-12  0.301995   0.308902    0.157690        0.321405   \n",
       "17   2018-10-13  0.300981   0.309499    0.127476        0.322055   \n",
       "16   2018-10-14  0.303756   0.309757    0.128370        0.322349   \n",
       "15   2018-10-15  0.334285   0.325730    0.308338        0.338883   \n",
       "14   2018-10-16  0.319500   0.325707    0.169924        0.338894   \n",
       "13   2018-10-17  0.315829   0.323006    0.170496        0.336139   \n",
       "12   2018-10-18  0.314121   0.319467    0.163594        0.332519   \n",
       "11   2018-10-19  0.310374   0.318876    0.149097        0.331945   \n",
       "10   2018-10-20  0.310579   0.320119    0.140709        0.333265   \n",
       "9    2018-10-21  0.313555   0.319762    0.135438        0.332930   \n",
       "8    2018-10-22  0.312916   0.320013    0.153044        0.333229   \n",
       "7    2018-10-23  0.311000   0.319416    0.154862        0.332652   \n",
       "6    2018-10-24  0.311810   0.320467    0.142621        0.333766   \n",
       "5    2018-10-25  0.310931   0.319445    0.134469        0.332745   \n",
       "4    2018-10-26  0.310608   0.319364    0.137640        0.332697   \n",
       "3    2018-10-27  0.311071   0.319659    0.141302        0.333036   \n",
       "2    2018-10-28  0.310810   0.319973    0.143483        0.333395   \n",
       "1    2018-10-29  0.310877   0.311936    0.175178        0.325123   \n",
       "0    2018-10-30  0.303846   0.312022    0.157590        0.325249   \n",
       "\n",
       "      bch_avg_block_size  bch_transactions  bch_mining_revenue  bch_accounts  \\\n",
       "1033            0.055634          0.000000            0.012875      0.000000   \n",
       "1032            0.055634          0.000000            0.011156      0.000000   \n",
       "1031            0.138178          0.057870            0.011759      0.001489   \n",
       "1030            0.138178          0.057870            0.013781      0.001489   \n",
       "1029            0.319659          0.210326            0.019561      0.001489   \n",
       "1028            0.319659          0.210326            0.017024      0.001489   \n",
       "1027            0.361094          0.180002            0.011350      0.004115   \n",
       "1026            0.361094          0.180002            0.014625      0.004115   \n",
       "1025            0.257161          0.181665            0.020794      0.004115   \n",
       "1024            0.257161          0.181665            0.017562      0.004115   \n",
       "1023            0.305948          0.186300            0.019600      0.008135   \n",
       "1022            0.305948          0.186300            0.019714      0.008135   \n",
       "1021            0.337240          0.225642            0.020967      0.008135   \n",
       "1020            0.337240          0.225642            0.014796      0.008135   \n",
       "1019            0.511714          0.203998            0.010617      0.011540   \n",
       "1018            0.511714          0.203998            0.006389      0.011540   \n",
       "1017            0.460682          0.229301            0.007959      0.011540   \n",
       "1016            0.460682          0.229301            0.008962      0.011540   \n",
       "1015            0.469945          0.272449            0.015609      0.013153   \n",
       "1014            0.469945          0.272449            0.010360      0.013153   \n",
       "1013            0.599769          0.287397            0.011957      0.013153   \n",
       "1012            0.599769          0.287397            0.009578      0.013153   \n",
       "1011            0.260769          0.256449            0.013890      0.014911   \n",
       "1010            0.260769          0.256449            0.016646      0.014911   \n",
       "1009            0.322734          0.288564            0.015503      0.014911   \n",
       "1008            0.322734          0.288564            0.015436      0.014911   \n",
       "1007            0.434422          0.340966            0.014351      0.016611   \n",
       "1006            0.434422          0.340966            0.012492      0.016611   \n",
       "1005            0.340409          0.328480            0.010522      0.016611   \n",
       "1004            0.340409          0.328480            0.014081      0.016611   \n",
       "...                  ...               ...                 ...           ...   \n",
       "29              0.623792          0.395497            0.241866      0.965986   \n",
       "28              0.623792          0.395497            0.234711      0.967425   \n",
       "27              0.596123          0.393782            0.201185      0.968374   \n",
       "26              0.596123          0.393782            0.209951      0.969132   \n",
       "25              0.779142          0.401798            0.220729      0.970550   \n",
       "24              0.779142          0.401798            0.189843      0.971500   \n",
       "23              0.586844          0.318888            0.181914      0.972805   \n",
       "22              0.586844          0.318888            0.207299      0.974315   \n",
       "21              0.783982          0.372411            0.198751      0.975159   \n",
       "20              0.783982          0.372411            0.190925      0.975971   \n",
       "19              0.717173          0.460686            0.210910      0.977460   \n",
       "18              0.717173          0.460686            0.222729      0.978381   \n",
       "17              0.630997          0.270332            0.182795      0.979765   \n",
       "16              0.630997          0.270332            0.195646      0.980471   \n",
       "15              0.750784          0.475545            0.156212      0.981232   \n",
       "14              0.750784          0.475545            0.235753      0.982717   \n",
       "13              0.905316          0.453702            0.198354      0.983912   \n",
       "12              0.905316          0.453702            0.203013      0.984982   \n",
       "11              0.859798          0.432016            0.201972      0.986300   \n",
       "10              0.859798          0.432016            0.205438      0.987439   \n",
       "9               0.574602          0.291594            0.206622      0.988874   \n",
       "8               0.574602          0.291594            0.200059      0.990106   \n",
       "7               0.733930          0.430811            0.225885      0.991321   \n",
       "6               0.733930          0.430811            0.223685      0.992511   \n",
       "5               0.885125          0.434580            0.160960      0.993774   \n",
       "4               0.885125          0.434580            0.197362      0.995012   \n",
       "3               0.706986          0.365400            0.206547      0.996388   \n",
       "2               0.706986          0.365400            0.194865      0.997879   \n",
       "1               0.809491          0.484865            0.200871      0.998593   \n",
       "0               0.809491          0.484865            0.208186      1.000000   \n",
       "\n",
       "      sp_close  dj_close  google_trends_bitcoin  \n",
       "1033  0.195031  0.158024               0.001304  \n",
       "1032  0.195031  0.158024               0.003176  \n",
       "1031  0.195031  0.158024               0.003644  \n",
       "1030  0.166638  0.133303               0.004112  \n",
       "1029  0.170314  0.134174               0.002708  \n",
       "1028  0.146305  0.111596               0.004112  \n",
       "1027  0.103488  0.076460               0.005516  \n",
       "1026  0.084372  0.061449               0.005516  \n",
       "1025  0.084372  0.061449               0.003176  \n",
       "1024  0.084372  0.061449               0.003644  \n",
       "1023  0.085861  0.066115               0.004112  \n",
       "1022  0.099485  0.076650               0.003644  \n",
       "1021  0.055552  0.043985               0.003176  \n",
       "1020  0.084199  0.064368               0.003176  \n",
       "1019  0.046520  0.029360               0.009729  \n",
       "1018  0.046520  0.029360               0.010197  \n",
       "1017  0.046520  0.029360               0.006452  \n",
       "1016  0.046520  0.029360               0.007388  \n",
       "1015  0.047428  0.031862               0.005984  \n",
       "1014  0.027458  0.009541               0.007388  \n",
       "1013  0.036227  0.019923               0.005984  \n",
       "1012  0.070638  0.038800               0.005048  \n",
       "1011  0.070638  0.038800               0.003176  \n",
       "1010  0.070638  0.038800               0.003176  \n",
       "1009  0.043570  0.020150               0.003176  \n",
       "1008  0.067670  0.045401               0.003176  \n",
       "1007  0.048898  0.025454               0.003176  \n",
       "1006  0.058348  0.036663               0.003176  \n",
       "1005  0.100901  0.072180               0.002708  \n",
       "1004  0.100901  0.072180               0.001304  \n",
       "...        ...       ...                    ...  \n",
       "29    0.994409  0.984135               0.037242  \n",
       "28    0.993355  0.995124               0.038824  \n",
       "27    0.995244  1.000000               0.037242  \n",
       "26    0.973549  0.982011               0.038824  \n",
       "25    0.958990  0.965855               0.037242  \n",
       "24    0.958990  0.965855               0.032496  \n",
       "23    0.958990  0.965855               0.027750  \n",
       "22    0.957955  0.969412               0.037242  \n",
       "21    0.954242  0.964379               0.037242  \n",
       "20    0.868318  0.889897               0.041988  \n",
       "19    0.816297  0.841017               0.056227  \n",
       "18    0.851480  0.866729               0.043570  \n",
       "17    0.851480  0.866729               0.034078  \n",
       "16    0.851480  0.866729               0.034078  \n",
       "15    0.836648  0.858720               0.048317  \n",
       "14    0.890321  0.907777               0.040406  \n",
       "13    0.889677  0.899562               0.038824  \n",
       "12    0.852978  0.870262               0.037242  \n",
       "11    0.852070  0.876072               0.038824  \n",
       "10    0.852070  0.876072               0.029332  \n",
       "9     0.852070  0.876072               0.030914  \n",
       "8     0.841268  0.864707               0.035660  \n",
       "7     0.827480  0.853427               0.037242  \n",
       "6     0.750697  0.798986               0.037242  \n",
       "5     0.795601  0.834903               0.035660  \n",
       "4     0.753048  0.808378               0.035660  \n",
       "3     0.753048  0.808378               0.030914  \n",
       "2     0.753048  0.808378               0.027750  \n",
       "1     0.737217  0.786405               0.038824  \n",
       "0     0.774778  0.825062               0.037242  \n",
       "\n",
       "[1034 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_csv('model_data.csv').iloc[:, 1:]\n",
    "# Fillimisht i kthej ne rend zbrites te dhenat\n",
    "model_data = model_data.assign(date=pd.to_datetime(model_data['date']))\n",
    "model_data = model_data.sort_values(by='date')\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head()\n",
    "model_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data[model_data['date']>=\"2016-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"split_data\"></a>Split Data (Testing, Training Data Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date='2018-04-25'\n",
    "# Heq kolonen e dates meqe nuk na duhet me\n",
    "training_set, test_set = model_data[model_data['date']<split_date], model_data[model_data['date']>=split_date]\n",
    "training_set = training_set.drop('date', 1)\n",
    "test_set = test_set.drop('date', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(training_set))\n",
    "# print(len(test_set)) \n",
    "# print(len(model_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kstruga/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "training = go.Scatter(\n",
    "                x=model_data[model_data['date'] < split_date]['date'].astype(datetime.datetime),\n",
    "                y=model_data[model_data['date'] < split_date]['btc_close'],\n",
    "                name = \"Trajnim\",\n",
    "                line = dict(color = '#6D13C1'),\n",
    "                opacity = 0.8)\n",
    "\n",
    "test = go.Scatter(\n",
    "                x=model_data[model_data['date'] >= split_date]['date'].astype(datetime.datetime),\n",
    "                y=model_data[model_data['date'] >= split_date]['btc_close'],\n",
    "                name = \"Testim\",\n",
    "                line = dict(color = '#17BECF'),\n",
    "                opacity = 0.8)\n",
    "\n",
    "\n",
    "data = [training, test]\n",
    "\n",
    "layout = dict(\n",
    "    title = \"Ndarja e te dhenave ne bashkesi per trajnim (80%) dhe testim (20%) \",\n",
    "    xaxis = dict(\n",
    "        range = ['2016-01-01','2018-10-30']),\n",
    "     yaxis=dict(\n",
    "        title='Cmimi Normalizuar i Bitcoin')\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename = \"Manually Set Range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"win_len_metrics\"></a>Set Window Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len=32\n",
    "pred_range=30 \n",
    "\n",
    "training_inputs = []\n",
    "for i in range(len(training_set)-window_len):\n",
    "    temp_set = training_set[i:(i+window_len)].copy()\n",
    "    training_inputs.append(temp_set)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"fill_training_test\"></a>Fill Training, Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    }
   ],
   "source": [
    "test_inputs = []\n",
    "for i in range(len(test_set)-window_len):\n",
    "    temp_set = test_set[i:(i+window_len)].copy()\n",
    "    test_inputs.append(temp_set)\n",
    "\n",
    "# print(test_inputs[0])\n",
    "test_outputs = test_set['btc_close'][window_len:].values\n",
    "print(len(test_outputs)) # predicting 45 points in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = [np.array(training_inputs) for training_inputs in training_inputs]\n",
    "training_inputs = np.array(training_inputs)\n",
    "\n",
    "test_inputs = [np.array(test_inputs) for test_inputs in test_inputs]\n",
    "test_inputs = np.array(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_outputs = []\n",
    "for i in range(window_len, len(training_set['btc_close'])-pred_range):\n",
    "    training_outputs.append(training_set['btc_close'][i:i+pred_range].values)\n",
    "    \n",
    "training_outputs = np.array(training_outputs)\n",
    "\n",
    "# testing outputs, which is needed to evaluate/predict the model\n",
    "testing_outputs = []\n",
    "for i in range(window_len, len(test_set['btc_close'])-pred_range):\n",
    "    testing_outputs.append(test_set['btc_close'][i:i+pred_range].values)\n",
    "\n",
    "testing_outputs = np.array(testing_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(813, 32, 11)\n",
      "(783, 30)\n",
      "(157, 32, 11)\n",
      "(127, 30)\n"
     ]
    }
   ],
   "source": [
    "print(training_inputs.shape)\n",
    "print(training_outputs.shape)\n",
    "\n",
    "print(test_inputs.shape)\n",
    "print(testing_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"load_model\"></a>Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def lstm_model(inputs, output_size, neurons, activ_func=\"relu\",\n",
    "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"train_model\"></a>Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "783/783 [==============================] - 1s 1ms/step - loss: 0.1000\n",
      "Epoch 2/100\n",
      "783/783 [==============================] - 0s 528us/step - loss: 0.0559\n",
      "Epoch 3/100\n",
      "783/783 [==============================] - 0s 513us/step - loss: 0.0474\n",
      "Epoch 4/100\n",
      "783/783 [==============================] - 0s 502us/step - loss: 0.0420\n",
      "Epoch 5/100\n",
      "783/783 [==============================] - 0s 502us/step - loss: 0.0405\n",
      "Epoch 6/100\n",
      "783/783 [==============================] - 0s 511us/step - loss: 0.0381\n",
      "Epoch 7/100\n",
      "783/783 [==============================] - 0s 529us/step - loss: 0.0352\n",
      "Epoch 8/100\n",
      "783/783 [==============================] - 0s 515us/step - loss: 0.0344\n",
      "Epoch 9/100\n",
      "783/783 [==============================] - 0s 507us/step - loss: 0.0333\n",
      "Epoch 10/100\n",
      "783/783 [==============================] - 0s 510us/step - loss: 0.0315\n",
      "Epoch 11/100\n",
      "783/783 [==============================] - 0s 550us/step - loss: 0.0317\n",
      "Epoch 12/100\n",
      "783/783 [==============================] - 0s 563us/step - loss: 0.0303\n",
      "Epoch 13/100\n",
      "783/783 [==============================] - 0s 500us/step - loss: 0.0294\n",
      "Epoch 14/100\n",
      "783/783 [==============================] - 0s 531us/step - loss: 0.0296\n",
      "Epoch 15/100\n",
      "783/783 [==============================] - 0s 554us/step - loss: 0.0279\n",
      "Epoch 16/100\n",
      "783/783 [==============================] - 0s 558us/step - loss: 0.0274\n",
      "Epoch 17/100\n",
      "783/783 [==============================] - 0s 541us/step - loss: 0.0277\n",
      "Epoch 18/100\n",
      "783/783 [==============================] - 0s 531us/step - loss: 0.0273\n",
      "Epoch 19/100\n",
      "783/783 [==============================] - 0s 531us/step - loss: 0.0291\n",
      "Epoch 20/100\n",
      "783/783 [==============================] - 0s 529us/step - loss: 0.0260\n",
      "Epoch 21/100\n",
      "783/783 [==============================] - 0s 496us/step - loss: 0.0265\n",
      "Epoch 22/100\n",
      "783/783 [==============================] - 0s 518us/step - loss: 0.0259\n",
      "Epoch 23/100\n",
      "783/783 [==============================] - 0s 524us/step - loss: 0.0256\n",
      "Epoch 24/100\n",
      "783/783 [==============================] - 0s 512us/step - loss: 0.0248\n",
      "Epoch 25/100\n",
      "783/783 [==============================] - 0s 514us/step - loss: 0.0251\n",
      "Epoch 26/100\n",
      "783/783 [==============================] - 0s 503us/step - loss: 0.0246\n",
      "Epoch 27/100\n",
      "783/783 [==============================] - 0s 518us/step - loss: 0.0254\n",
      "Epoch 28/100\n",
      "783/783 [==============================] - 0s 501us/step - loss: 0.0272\n",
      "Epoch 29/100\n",
      "783/783 [==============================] - 0s 526us/step - loss: 0.0254\n",
      "Epoch 30/100\n",
      "783/783 [==============================] - 0s 561us/step - loss: 0.0235\n",
      "Epoch 31/100\n",
      "783/783 [==============================] - 0s 505us/step - loss: 0.0232\n",
      "Epoch 32/100\n",
      "783/783 [==============================] - 0s 513us/step - loss: 0.0235\n",
      "Epoch 33/100\n",
      "783/783 [==============================] - 0s 560us/step - loss: 0.0227\n",
      "Epoch 34/100\n",
      "783/783 [==============================] - 0s 540us/step - loss: 0.0229\n",
      "Epoch 35/100\n",
      "783/783 [==============================] - 0s 514us/step - loss: 0.0228\n",
      "Epoch 36/100\n",
      "783/783 [==============================] - 0s 525us/step - loss: 0.0228\n",
      "Epoch 37/100\n",
      "783/783 [==============================] - 0s 513us/step - loss: 0.0226\n",
      "Epoch 38/100\n",
      "783/783 [==============================] - 0s 508us/step - loss: 0.0230\n",
      "Epoch 39/100\n",
      "783/783 [==============================] - 0s 494us/step - loss: 0.0221\n",
      "Epoch 40/100\n",
      "783/783 [==============================] - 0s 528us/step - loss: 0.0233\n",
      "Epoch 41/100\n",
      "783/783 [==============================] - 0s 515us/step - loss: 0.0240\n",
      "Epoch 42/100\n",
      "783/783 [==============================] - 0s 549us/step - loss: 0.0229\n",
      "Epoch 43/100\n",
      "783/783 [==============================] - 0s 525us/step - loss: 0.0210\n",
      "Epoch 44/100\n",
      "783/783 [==============================] - 0s 500us/step - loss: 0.0217\n",
      "Epoch 45/100\n",
      "783/783 [==============================] - 0s 512us/step - loss: 0.0220\n",
      "Epoch 46/100\n",
      "783/783 [==============================] - 0s 528us/step - loss: 0.0220\n",
      "Epoch 47/100\n",
      "783/783 [==============================] - 0s 554us/step - loss: 0.0225\n",
      "Epoch 48/100\n",
      "783/783 [==============================] - 0s 539us/step - loss: 0.0214\n",
      "Epoch 49/100\n",
      "783/783 [==============================] - 0s 539us/step - loss: 0.0216\n",
      "Epoch 50/100\n",
      "783/783 [==============================] - 0s 522us/step - loss: 0.0212\n",
      "Epoch 51/100\n",
      "783/783 [==============================] - 0s 636us/step - loss: 0.0211\n",
      "Epoch 52/100\n",
      "783/783 [==============================] - 1s 658us/step - loss: 0.0220\n",
      "Epoch 53/100\n",
      "783/783 [==============================] - 1s 670us/step - loss: 0.0212\n",
      "Epoch 54/100\n",
      "783/783 [==============================] - 1s 658us/step - loss: 0.0203\n",
      "Epoch 55/100\n",
      "783/783 [==============================] - 1s 662us/step - loss: 0.0225\n",
      "Epoch 56/100\n",
      "783/783 [==============================] - 1s 662us/step - loss: 0.0219\n",
      "Epoch 57/100\n",
      "783/783 [==============================] - 1s 672us/step - loss: 0.0207\n",
      "Epoch 58/100\n",
      "783/783 [==============================] - 1s 670us/step - loss: 0.0199\n",
      "Epoch 59/100\n",
      "783/783 [==============================] - 1s 649us/step - loss: 0.0204\n",
      "Epoch 60/100\n",
      "783/783 [==============================] - 1s 670us/step - loss: 0.0201\n",
      "Epoch 61/100\n",
      "783/783 [==============================] - 1s 655us/step - loss: 0.0212\n",
      "Epoch 62/100\n",
      "783/783 [==============================] - 1s 662us/step - loss: 0.0207\n",
      "Epoch 63/100\n",
      "783/783 [==============================] - 1s 655us/step - loss: 0.0207\n",
      "Epoch 64/100\n",
      "783/783 [==============================] - 1s 660us/step - loss: 0.0213\n",
      "Epoch 65/100\n",
      "783/783 [==============================] - 1s 665us/step - loss: 0.0202\n",
      "Epoch 66/100\n",
      "783/783 [==============================] - 1s 665us/step - loss: 0.0203\n",
      "Epoch 67/100\n",
      "783/783 [==============================] - 1s 684us/step - loss: 0.0197\n",
      "Epoch 68/100\n",
      "783/783 [==============================] - 1s 665us/step - loss: 0.0193\n",
      "Epoch 69/100\n",
      "783/783 [==============================] - 1s 657us/step - loss: 0.0193\n",
      "Epoch 70/100\n",
      "783/783 [==============================] - 1s 667us/step - loss: 0.0196\n",
      "Epoch 71/100\n",
      "783/783 [==============================] - 1s 667us/step - loss: 0.0202\n",
      "Epoch 72/100\n",
      "783/783 [==============================] - 1s 690us/step - loss: 0.0191\n",
      "Epoch 73/100\n",
      "783/783 [==============================] - 1s 661us/step - loss: 0.0196\n",
      "Epoch 74/100\n",
      "783/783 [==============================] - 1s 664us/step - loss: 0.0196\n",
      "Epoch 75/100\n",
      "783/783 [==============================] - 1s 658us/step - loss: 0.0196\n",
      "Epoch 76/100\n",
      "783/783 [==============================] - 1s 659us/step - loss: 0.0195\n",
      "Epoch 77/100\n",
      "783/783 [==============================] - 1s 678us/step - loss: 0.0186\n",
      "Epoch 78/100\n",
      "783/783 [==============================] - 1s 672us/step - loss: 0.0193\n",
      "Epoch 79/100\n",
      "783/783 [==============================] - 1s 657us/step - loss: 0.0185\n",
      "Epoch 80/100\n",
      "783/783 [==============================] - 1s 710us/step - loss: 0.0186\n",
      "Epoch 81/100\n",
      "783/783 [==============================] - 1s 682us/step - loss: 0.0179\n",
      "Epoch 82/100\n",
      "783/783 [==============================] - 1s 663us/step - loss: 0.0194\n",
      "Epoch 83/100\n",
      "783/783 [==============================] - 1s 677us/step - loss: 0.0185\n",
      "Epoch 84/100\n",
      "783/783 [==============================] - 1s 678us/step - loss: 0.0182\n",
      "Epoch 85/100\n",
      "783/783 [==============================] - 1s 673us/step - loss: 0.0188\n",
      "Epoch 86/100\n",
      "783/783 [==============================] - 1s 685us/step - loss: 0.0200\n",
      "Epoch 87/100\n",
      "783/783 [==============================] - 1s 672us/step - loss: 0.0191\n",
      "Epoch 88/100\n",
      "783/783 [==============================] - 1s 669us/step - loss: 0.0181\n",
      "Epoch 89/100\n",
      "783/783 [==============================] - 1s 689us/step - loss: 0.0185\n",
      "Epoch 90/100\n",
      "783/783 [==============================] - 1s 667us/step - loss: 0.0190\n",
      "Epoch 91/100\n",
      "783/783 [==============================] - 1s 659us/step - loss: 0.0185\n",
      "Epoch 92/100\n",
      "783/783 [==============================] - 1s 681us/step - loss: 0.0184\n",
      "Epoch 93/100\n",
      "783/783 [==============================] - 1s 675us/step - loss: 0.0185\n",
      "Epoch 94/100\n",
      "783/783 [==============================] - 1s 677us/step - loss: 0.0188\n",
      "Epoch 95/100\n",
      "783/783 [==============================] - 1s 666us/step - loss: 0.0183\n",
      "Epoch 96/100\n",
      "783/783 [==============================] - 1s 669us/step - loss: 0.0179\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 1s 652us/step - loss: 0.0178\n",
      "Epoch 98/100\n",
      "783/783 [==============================] - 1s 650us/step - loss: 0.0176\n",
      "Epoch 99/100\n",
      "783/783 [==============================] - 0s 621us/step - loss: 0.0184\n",
      "Epoch 100/100\n",
      "783/783 [==============================] - 1s 639us/step - loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "# random seed for reproducibility\n",
    "np.random.seed(202)\n",
    "\n",
    "# initialise model architecture\n",
    "bt_model = lstm_model(training_inputs, output_size=pred_range, neurons = 100)\n",
    "# bt_model = denser_model(LSTM_training_inputs, output_size=pred_range, neurons = 100)\n",
    "\n",
    "# print(bt_model.get_weights())\n",
    "# train model on data\n",
    "bt_history = bt_model.fit(training_inputs[:-pred_range], training_outputs, \n",
    "                            epochs=100, batch_size=32, verbose=1, shuffle=True)\n",
    "\n",
    "loss_fn=str(np.mean(bt_history.history['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz;\n",
    "from keras.utils.vis_utils import plot_model\n",
    "graph = plot_model(bt_model, to_file=\"my_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAGGCAYAAABsV2YRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3XeYXHX1x/H3mZndTe+bQHqFFIEAIUDoTYpA0B9dpIMoYAFRUUBAsKAIKigiQSlKFZEmHUJASKWlkt57b5tt5/fHvbuZTGZ2Z5LMzmT283qefTL3zi3n7uSZs99u7o6IiEi6IrkOQEREdi9KHCIikhElDhERyYgSh4iIZESJQ0REMqLEISIiGVHiEBGRjChxSF4ys/PNbJyZbTCzxWb2XzM7PNdx7W7M7F0zKwt/jzU/L+Y6Ltm9KXFI3jGz64B7gV8AnYDuwJ+A4bmMK56ZxXIdQwaucfcWcT+nJTso2TNl+py72e9FdpASh+QVM2sN3A5c7e7PuftGd69w9xfd/YbwmBIzu9fMFoU/95pZSfje0Wa2wMyuN7NlYWnlkvC9g81siZlF4+73VTP7LHwdMbMfm9lMM1tpZk+bWbvwvZ5m5mZ2mZnNA94O919oZnPD4282szlmdnwG17vIzOaZ2Qoz+2lcXFEz+0l47nozG29m3cL3+pvZG2a2ysymmdnZO/i7rvld/cjMlgB/S7YvPPYKM5sR3vMFM+scdx03s6vNbDowfUdikd2LEofkm0OBJsC/6zjmp8AhwGBgP2AocFPc+3sArYEuwGXA/WbW1t1HAxuBY+OOPR/4Z/j6WuAM4CigM7AauD/h3kcBA4ATzWwgQUno68Cecfeskc71Dgf2Bo4DbjGzAeH+64DzgFOAVsClwCYzaw68EcbcETgX+FMYy47YA2gH9ACuTLbPzI4FfgmcHT7nXODJhOucARwM7Ggcsjtxd/3oJ29+CL6El9RzzEzglLjtE4E54eujgc1ALO79ZcAh4es7gIfD1y0JEkmPcHsKcFzceXsCFUAM6Ak40Dvu/VuAJ+K2mwHlwPEZXK9r3PtjgHPD19OA4Ume/RxgVMK+vwA/S/G7ehfYBKyJ+/l53O+qHGgSd3yyfSOAu+K2W4TP0TPcduDYXP/f0U/D/ag+UvLNSqCDmcXcvTLFMZ0J/uqtMTfcV3uNhHM3EXzZQfCX+v/M7FvA14AJ7l5zrR7Av82sOu7cKoJ2lhrzE+Ko3Xb3TWa2Mu79dK63JEWc3QgSZKIewMFmtiZuXwx4LMmxNb7j7g+leG+5u5fVs68zMKFmw903hM/ZBZgT7o7/vUiBU1WV5JsPgS0EVR+pLCL4Aq3RPdxXL3efTJBoTmbbaioIvvxOdvc2cT9N3H1h/CXiXi8GutZsmFlToH2G10tlPtAnxf6RCdds4e7fSuOaySSbHjtx3za/77C6rD2Q6vciBU6JQ/KKu68lqAK638zOMLNmZlZkZieb2V3hYU8AN5lZqZl1CI9/PIPb/BP4LnAk8Ezc/geAO82sB0B4/bp6cj0LnGZmw8ysGLgVsJ24XryHgJ+bWT8L7Gtm7YGXgL3M7Bvh76XIzA6KaxvJhieAS8xscNgJ4RfAaHefk8V7Sh5T4pC84+53EzQO3wQsJ/gr+xrg+fCQO4BxwGfA5wTVKHdkcIsnCBqs33b3FXH7fw+8ALxuZuuBjwgafFPFOYmgAfxJgtLHBoL2lC07cr0EvwOeBl4H1hG0MzR19/XAlwkaxRcRVHX9Giip41r32bbjOManGQMA7v4mcDPwL4Ln7BPeXxopc1cJU2RXMLMWBI3P/dx9dq7jEckWlThEdoKZnRZWpzUHfktQApqT26hEskuJQ2TnDCeoMloE9CPoTqtivBQ0VVWJiEhGVOIQEZGMKHGIiEhGCnLkeIcOHbxnz565DkNEZLcyfvz4Fe5eWt9xBZk4evbsybhx43IdhojIbsXM5tZ/lKqqREQkQ0ocIiKSESUOERHJiBKHiIhkRIlDREQyosQhIiIZUeIQEZGMKHGIiEhGlDhERCQjDZY4zOwkM5tmZjPM7MdJ3j/SzCaYWaWZnZnw3kVmNj38uaihYhYRke01SOIwsyhwP3AyMBA4z8wGJhw2D7iYYD3o+HPbAT8jWHJzKPAzM2ub7ZhFRCS5hipxDAVmuPssdy8nWKN5ePwB7j7H3T8DqhPOPRF4w91Xuftq4A3gpGwEuXZzBe9MXcaKDVvqP1hEpJFqqMTRBZgft70g3LfLzjWzK81snJmNW758+Q4FOXvFRi75+1g+W7Bmh84XEWkMCqZx3N0fdPch7j6ktLTeWYGTikUMgIoqrYooIpJKQyWOhUC3uO2u4b5sn5uRomjw66hU4hARSamhEsdYoJ+Z9TKzYuBc4IU0z30N+LKZtQ0bxb8c7tvlYtGgxFFZndjMIiIiNRokcbh7JXANwRf+FOBpd59kZreb2ekAZnaQmS0AzgL+YmaTwnNXAT8nSD5jgdvDfbtcUST4daiqSkQktQZbAdDdXwFeSdh3S9zrsQTVUMnOfRh4OKsBElfiqFKJQ0QklYJpHN8VahJHRbVKHCIiqShxxKmpqlKJQ0QkNSWOONHaqiqVOEREUlHiiFPbOK5eVSIiKSlxxImpxCEiUi8ljjg1I8cr1TguIpKSEkccMyMWMTWOi4jUQYkjQSxqKnGIiNRBiSNBUSRChUocIiIpKXEkiEVNjeMiInVQ4kgQi0Y0yaGISB2UOBIURUyTHIqI1EGJI0EsGlGvKhGROihxJIhFTZMciojUQYkjQVFEJQ4RkboocSRQryoRkbopcSSIRSOqqhIRqYMSR4IiTTkiIlInJY4EqqoSEambEkeComhE63GIiNRBiSNBMDuuShwiIqkocSSIRTXJoYhIXZQ4EhRpWnURkTopcSSIaQCgiEidlDgSxKKa5FBEpC5KHAliEdO06iIidVDiSBDMjqsSh4hIKkocCYL1OFTiEBFJRYkjQbACoEocIiKpKHEkiKk7rohInZQ4Emg9DhGRuilxJIhFjWqHapU6RESSUuJIUBQNfiWa6FBEJDkljgSxiAGoS66ISApKHAliYYlDiUNEJDkljgRF0aDEoaoqEZHklDgSxCIqcYiI1EWJI0GspsShLrkiIkkpcSSoqarSIEARkeSUOBJsrapSiUNEJBkljgS1jeNq4xARSUqJI0FtiUO9qkREklLiSBBTiUNEpE5KHAmKomrjEBGpixJHgtopR9SrSkQkKSWOBDVTjmgch4hIckocCWrHcaiNQ0QkKSWOBNHaqiqVOEREklHiSFC7HodKHCIiSSlxJIipxCEiUicljgQqcYiI1E2JI0FMjeMiInVS4khQM+VIlaqqRESSUuJIoEkORUTqpsSRoHbNcZU4RESSUuJIUNOrSiUOEZHklDgSbJ3kUIlDRCQZJY4E0YhhpqoqEZFUlDiSKIpEVFUlIpKCEkcSsahpPQ4RkRSUOJKIRUzrcYiIpKDEkURRNKL1OEREUlDiSCKoqlKJQ0QkmQZLHGZ2kplNM7MZZvbjJO+XmNlT4fujzaxnuL/IzB4xs8/NbIqZ3ZjtWGORCBXqVSUiklSdicPMOpjZdWb2lpmtMLOK8N+3zOwHZlaazk3MLArcD5wMDATOM7OBCYddBqx2977APcCvw/1nASXuvg9wIPDNmqSSLUUqcYiIpJQycZjZr4CPgb2BEcAJwIDw3xFAP2BCeFx9hgIz3H2Wu5cDTwLDE44ZDjwSvn4WOM7MDHCguZnFgKZAObAuvcfbMbFoROM4RERSiNXx3gKgr7tvSfLex8A/zawJcHka9+kCzE+49sGpjnH3SjNbC7QnSCLDgcVAM+D77r4q8QZmdiVwJUD37t3TCCm1WMQ0jkNEJIWUJQ53vy9F0og/pszd79v1YW1jKFAFdAZ6AdebWe8ksTzo7kPcfUhpaVo1aCkVRSMaxyEikkJ9bRw/SNg+IWH7d2neZyHQLW67a7gv6TFhtVRrYCVwPvCqu1e4+zLgA2BImvfdIbGoxnGIiKRSX6+qWxK2n0rYTqeaCmAs0M/MeplZMXAu8ELCMS8AF4WvzwTedncH5gHHAphZc+AQYGqa990hwZQjKnGIiCRTX+KwDLeTcvdK4BrgNWAK8LS7TzKz283s9PCwEUB7M5sBXAfUdNm9H2hhZpMIEtDf3P2zdO67o6IR9aoSEUmlrsZxCHo0ZbKd+kLurwCvJOy7Je51GUHX28TzNiTbn02xqLGpQolDRCSZ+hKHmVkvtpYsIgnbaZU4djdqHBcRSa2+xNEcmMG2CWJm9sLJDzFVVYmIpFRn4nD3RjmXVVFUU46IiKSyw4nBzAaZ2V27Mph8oUkORURSyyhxhHNXfcfMxgOfEcw7VXBikQhVGschIpJUfW0cmFkRcBrBGIuTCKYF6Qwc5O4TshtebhRFTeM4RERSqG/k+P0Ec0TdD8wFjgpnr11LMN9UQdLIcRGR1OorcVwFrAJuBZ5097VZjygPxDRyXEQkpfraOPoAfwRuAJaY2b/M7P/SOG+3pvU4RERSqzMBuPscd789rJ76MkHpYwRQCtyZZDGmgqD1OEREUku75ODuo9z9CmAP4AKgO/BJtgLLpaJwPY5gjkUREYmXcZVTuAbHP939RKDnrg8p92LR4NeiLrkiIturs3HczBKnVU/m9l0US96IRYMZViqrnVg0x8GIiOSZ+npV3QpMI5jOPNmEhgX5J3lRJChxVFRV06RImUNEJF59ieP7wIXAgcCjwOPunrhyX8GpLXGoZ5WIyHbq61X1e3c/kGA9jHbA/8zsDTO7wMxKGiTCHKhp49BEhyIi20urcdzdJ7v7jwjGdXwM/B04LItx5VRRRCUOEZFU0kocZjbAzH5FsDbHgcBlwP+yGVgu1ZQ4lDhERLZXX6+qawnaOJoBjwFHuPv8hggsl4rCNg5VVYmIbK++xvHfE/SqGkcwhfqdZtt2rnL3C7MTWu7EIipxiIikUl/iuJ0C7XJbl2jYxqGJDkVEtlff0rG3NlAceaUobgCgiIhsK2XjuJntl84F0j1ud7K1cVwlDhGRRHWVOO43s3UEjeIj3X1RzRtmtidwFEHDeUvgiKxG2cCKaquqVOIQEUmUMnG4++FmdirBYk4jzKwKWE+QKAx4E7jP3V9pkEgbUG2JQ72qRES2U18bx0vAS+G64/2ANsBqYIa7VzRAfDmhKUdERFKrr1cVAGGSmJzlWPJG/CSHIiKyrYJeAnZH1ZQ4tB6HiMj2lDiS2DpyXIlDRCSREkcSW0eOq6pKRCRRvYnDzKJmNrOQp1FPpMZxEZHU6k0c7l4FVAFNsh9OfijSehwiIiml1asKuBd42sx+ASwgbv4qd5+VjcByKab1OEREUko3cdwX/ntCwn4HCm5R7toVANXGISKynXTHcTSqRnRNcigiklq6JQ4AzKw70AVYUMgLOqlXlYhIaukuHbunmY0kWDr2OWCmmb1nZp2zGl2O1I7jUBuHiMh20q2C+jPwKdDW3fcE2gIfAw9kK7BcMjOiEdMkhyIiSaRbVXU4sGfNxIbuvtHMfggszFpkORaLmHpViYgkkW6JYzXBmuPx9gbW7Npw8kdRNKKqKhGRJNItcdwFvGlmI4C5QA/gEuDmbAWWa7GoqqpERJJJtzvuX81sJnA+sC+wCDjf3d/KZnC5FIuoxCEikky9icPMosDDwJXu/nb2Q8oPRVFTd1wRkSTSnavqy0Cj+hYNelWpxCEikijdxvF7gNvCJWQbhaBxvFHlShGRtKTbOH4tsAdwnZktZ9tJDrtnI7BcU3dcEZHk0k0cF2Q1ijwUi0bUq0pEJIl0G8cvJWgc35L9kPJDUdTUq0pEJAk1jqcQ05QjIiJJqXE8hZhGjouIJKXG8RSKosbm8qpchyEiknfUOJ5CLBKhqroy12GIiOSddKccGZntQPKNGsdFRJKrs43DzP6TsH1bwvbYbASVD2IRdccVEUmmvsbxYxK2r03Y7r8LY8krsagGAIqIJJNur6oalrBdsN+sRdEIFSpxiIhsJ9PEUbCJIpGmHBERSa6+xvEiM7uErSWNEjO7NIPzd1saxyEiklx9X/yjgQvjtscA30h4vyAVaQVAEZGk6kwc7n50A8WRd2KRiKqqRESSyLSNo9EIxnGoxCEikkiJI4VYVCsAiogko8SRQjDliOOu5CEiEq/BEoeZnWRm08xshpn9OMn7JWb2VPj+aDPrGffevmb2oZlNMrPPzaxJtuMtigYdydSzSkRkW2knDjPrb2Y3m9n9cdv7pnluFLgfOBkYCJxnZgMTDrsMWO3ufQmmcf91eG4MeBy4yt0HAUcDFenGvaNi0eBXo55VIiLbSitxmNlZwHtAF7Z2x20B/C7N+wwFZrj7LHcvB54EhiccMxx4JHz9LHCcmRnBIlKfufunAO6+MlxcKqtiEZU4RESSSbfEcTtwgrtfBdR8aX8K7Jfm+V2A+XHbC8J9SY9x90pgLdAe2AtwM3vNzCaY2Q+T3cDMrjSzcWY2bvny5WmGlVpN4qhUzyoRkW2kmzg6Ap+Frz3u34b4czwGHA58Pfz3q2Z2XOJB7v6guw9x9yGlpaU7f9PaqiqVOERE4qWbOMaz7YhxgHMJRpKnYyHQLW67a7gv6TFhu0ZrYCVB6eQ9d1/h7puAV4AD0rzvDtvaOK4Sh4hIvHQTx3eAO8xsJNDczF4Dfg58P83zxwL9zKyXmRUTJJ0XEo55AbgofH0m8LYHfWFfA/Yxs2ZhQjkKmJzmfXdYLBKWONTGISKyjXRXAJxqZv2BU4GXCNoiXnL3DWmeX2lm1xAkgSjwsLtPMrPbgXHu/gIwAnjMzGYAqwiSC+6+2sx+R5B8HHjF3V/O6Cl3QCwscahXlYjIttJKHGb2B3f/DvB0wv573f176VzD3V8hqGaK33dL3Osy4KwU5z5O0CW3wRSFbRzqVSUisq10q6ouTrE/sd2jYGztVaXEISISr84SR9zaG7GEdTgAegMrshJVHqgtcaiqSkRkG/VVVdWUKIrZtnThwFK2NmYXnJo2jip1xxUR2UZ963EcA2Bmd7j7TQ0TUn6o6VWl7rgiIttKd+nXW8wsaXuIuxfkN2vNOA61cYiIbCvdxFFJ6lHi0V0US17RJIciIsmlmzh6JWzvCfwYeHHXhpM/NMmhiEhy6Q4AnJuwa66ZXUQwKG/ELo8qD9T0qlJVlYjItnZmIadWwM7PJpinNHJcRCS5dEeOP8a2bRzNgCNp4NHcDalJUdB0s6k860t/iIjsVtJt45iRsL0ReMDd39zF8eSNDi2KAVi2bkuOIxERyS/ptnHclu1A8k1JLEq75sUsXV+W61BERPJKysSRZIqRpNz94V0XTn7p2LJEJQ4RkQR1lTjSmcDQgYJNHJ1aNWGZShwiIttImThqphtpzDq1KmHqknW5DkNEJK+k2ziOmbUFTgO6ECzz+qK7r85WYPmgY8smLF+/hapqJxoOCBQRaezSGsdhZocCM4GrgH2BbwIzw/0Fq1OrEqodVm5QO4eISI10Sxz3At929ydrdpjZOcAfgIOyEVg+6NiqCQBL122pfS0i0tilO3J8LxKWjQWeBfru2nDyS6cwWaiBXERkq3QTx3Tg3IR9ZxFUXxWsTq1KgKDEISIigXSrqr4HvGRm3wHmAj2BfsCpWYorL3RoUYIZLF2nEoeISI10R47/z8z6AF8BOhNMp/6Ku6/KZnC5VhSN0L55iaqqRETipN0dN+x6+ziAmfUmmB23oBMHBKPHVVUlIrJVut1xnzCzYeHrS4BJwCQzuyybweWDTq1KVFUlIhIn3cbx44Bx4evrgOOBoQSrABa0YNoRlThERGqkW1VV7O7lZtYFaOfuHwCYWafshZYfOrZqwooNW6isqq5dh1xEpDFLN3F8YmY3Aj2AlwHCJFLwEzl1alWCO6zYUM4erTUIUEQk3T+hLwP2AZoCN4f7DgX+kY2g8kmnljWjx9XOISIC6XfHnQmcn7DvWYLR4wWtY+0gQCUOERFIv8SBmV1qZm+Y2aTw38vMrOCnjK2ZdmSpGshFRIA0SxxmdhcwnGCyw7lAd+AHwN7AD7MWXR5o37yYiMFylThERID0G8cvBg5w9wU1O8zsZWACBZ44YtEIHVpoEKCISI10q6rWhz+J+wq+VxUE1VVLNe2IiAhQR4kjnFakxr3Ac2b2K2AB0A24Abgnu+Hlh06tSli4RolDRATqrqqaATgQ3wCeuA75scB9uzqofFPasgkfz1uT6zBERPJCysTh7homHerUqoSVG8spr6ymOKZfi4g0bjv8LWhmg8LeVgWvpkvuCq09LiKSWeIwsw5m9h0zGw98BgzMTlj5pZMGAYqI1Kq3O66ZFQGnARcBJwHzCRZzOsjdJ2Q3vPzQsXbaEZU4RETqLHGY2f3AYuB+goF/R7l7X2AtQe+qRqF29LhKHCIi9ZY4riJY5e9W4El3X5v1iPJQhxbFNCuOMnvFxlyHIiKSc/W1cfQB/kgwZmOJmf3LzP4vjfMKipnRp7QFM5dvyHUoIiI5V2cCcPc57n57WD31ZYLSxwigFLjTzBpF4zhA344tmLFMiUNEJO2Sg7uPcvcrgD2ArxOMHv8kW4Hlm74dW7B4bRkbtlTmOhQRkZzKuMrJ3cvc/Ql3PwnouetDyk99SlsAMFOlDhFp5HaqrcLdF+2qQPJd345B4lB1lYg0do2qkXtn9GjfjFjEmKEGchFp5JQ40lQUjdCzQ3OVOESk0VPiyEDf0hZq4xCRRi/dpWPbESwVOxhoEf+eux+ZhbjyUt+OLXhjylLNkisijVq6S8f+EygBngY2ZS+c/Na3Ywuqqp05KzeyV6eWuQ5HRCQn0k0cw4BSd2/Us/zF96xS4hCRxird+pbPgK7ZDGR30Lu0OaAuuSLSuKVb4ngbeNXM/gYsiX/D3R/e5VHlqWbFMbq0aarEISKNWrqJ4wiCadRPSNjvQKNJHBBUV2myQxFpzNJKHO5+TLYD2V307diC0bNXUl3tRCKW63BERBpcxn1KLRCp+clGUPmsb8cWlFVUs3DN5lyHIiKSE2l98ZtZFzP7t5mtBCqBirifRqW2Z5Wqq0SkkUq3xPAAUA4cB2wADgBeIFghsFHpG86SO23J+hxHIiKSG+kmjmHApe7+CeDu/ilwGXB91iLLU22bF7Nv19Y8+r85lFVU5TocEZEGl27iqCKoogJYY2alwEagS1aiynM/OWUAi9aW8dCoWbkORUSkwaWbOEYDp4SvXwOeAp4DxmUjqHx3SO/2nDioE396dybL1pXlOhwRkQaVbuL4BjAyfP09ggGBE4HzsxHU7uDGkwdQUVXNb1+flutQREQaVFqJw93XuPuq8PVmd7/D3X/k7ovTvZGZnWRm08xshpn9OMn7JWb2VPj+aDPrmfB+dzPbYGY/SPee2dSzQ3MuHtaTZ8YvYOLCtbkOR0SkwaTbHbfEzO40s1lmtjbc92UzuybN86PA/cDJwEDgPDMbmHDYZcBqd+8L3AP8OuH93wH/Ted+DeWaY/vRumkRfx45M9ehiIg0mHSrqu4BvgR8nWCaEYBJwLfSPH8oMMPdZ7l7OfAkMDzhmOHAI+HrZ4HjzMwAzOwMYHZ4z7zRumkRp+/XmbemLGXDlsr6TxARKQDpJo6vAue7+4dANYC7LyT9XlVdgPlx2wuSnFt7jLtXAmuB9mbWAvgRcFtdNzCzK81snJmNW758eZph7bzT9+tMWUU1b0xeUv/BIiIFIN3EUU7CvFZhl9yVuzyi7d0K3OPudQ7VdvcH3X2Iuw8pLS1tgLACB3RvS5c2TfnPJ4sa7J4iIrmUbuJ4BnjEzHoBmNmewH0EVU7pWAh0i9vuGu5LeoyZxYDWBInpYOAuM5tD0KPrJ+m2rTSESMQ4bb/OjJq+gpUbGvU6VyLSSKSbOH5C0MbwOdAGmA4sop7qozhjgX5m1svMioFzCaYsifcCcFH4+kzgbQ8c4e493b0ncC/wC3e/L837NojhgztTVe28MlHVVSJS+NLtjlvu7t939xZAJ6BluF2e5vmVwDUEgwenAE+7+yQzu93MTg8PG0HQpjEDuA7Yrstuvuq/R0v6dWzBC58kFqJERApPnetxmFn3FG91Czs84e7z0rmRu78CvJKw75a412XAWfVc49Z07tXQzIzhgzvz29e/YOGazXRp0zTXIYmIZE19JY45BFVUs8PXiT+zsxTXbue0/ToD8IIayUWkwNWXOD4laM+4CegBFCX8FGc1ut1Ij/bN2a9ra16bpHYOESlsdSYOd9+foKG6HfABQVXTuUCxu1e5u+YVj3PcgE58umANy9erd5WIFK56G8fdfaK73wD0JJj241RgsZkdkOXYdjvH9u+IO7w7bVmuQxERyZpM1gzvBxwFHAp8DKzOSkS7sUGdW9GpVQlvT1XiEJHCVWfiMLN2Zna1mY0BnidYNvZIdz/G3dUwnsDMOLZ/J0ZNX0F5ZXWuwxERyYo6u+MSDPKbDTwGfBTu62tmfWsOcPe3sxTbbunY/h15Ysw8xs5ZxWF9O+Q6HBGRXa6+xLEEaAJcEf4kcqD3rg5qd3ZY3/YUxyK8NWWZEoeIFKQ6E0c4zYdkoFlxjGF92vPW1KXcfOoAagZKiogUikwaxyVNx/XvyNyVm5i1YmOuQxER2eWUOLLgmP4dAXh7inpXiUjhUeLIgq5tm9F/j5Y88uEclqwty3U4IiK7lBJHlvzia/uwZlMF5z74oZKHiBQUJY4sOaB7Wx65dCgrNpRz3l8/UvIQkYKhxJFFB/ZoyyOXHsTy9Vu4YMRo1pVV5DokEZGdpsSRZQf2aMdfLxzCnBUb+d6Tn1BV7bkOSURkpyhxNIBD+7Tn1tMH8fbUZfzmtWm5DkdEZKfUN3JcdpELDunBlMXreGDkTPrv0ZIz9u+S65BERHaIShwN6NbTBzG0Vztuen4iazervUNEdk9KHA2oKBrh1tMGsWFLJY99OCfX4YiI7BAljgY2sHMrjtm7lIc/mMPmci2gKCK7HyWOHLj6mL6s2ljOk2Pn5ToUEZGMKXHkwJCe7Rjaqx0PvjdLCz6JyG5HiSNHrj6mL4vXlvH8xwtZsraMkV8s11rlIrJbUHfcHDmyXwc2uS+6AAAchUlEQVQGdW7Fj577DI8bE/jPyw9mmBaAEpE8phJHjpgZd351H75xSA9uO30Q/7ziYLq3a8bN/5mo6isRyWsqceTQ4G5tGNytTe32racP5NK/j+PhD2Zz1VF9chiZiEhqKnHkkWP7d+L4AZ34/ZvTWbRmc67DERFJSokjz/zstIE4zs9fmpzrUEREklLiyDPd2jXj2mP78d+JS/jd69Nw12y6IpJf1MaRh646qg9zV27kD2/PYHNFFT85ZQBmBkBlVTWxqPK9iOSOEkceikaMX31tX5oURfnrqNksWbeFkliE8XNXM2/VJv55+cEc3Lt9rsMUkUZKf7rmqUjEuO30QXzzyN68+Oki3pqylD6lLWjbrJjf7mAVVllFFZVV6uorIjtHJY48ZmbceMoAvn10X1o1jWFmPPrhHG75zyQ+mLGSw/ulP1CwoqqaU/4wisP7duD24V/KXtAiUvBU4tgNtG5WVNvGcc5B3dizdRPuefOLjEodL366iFnLN/LqxCVqcBeRnaLEsZspiUW5+pi+jJ+7mvemr0jrnOpq54GRM4lGjGXrt/DF0g1ZjlJECpkSx27o7CHd6NKmKfe8kV6p4+2py/hi6QauO2EvAEZNX57tEEWkgClx7IaKYxGuPqYvn8xfwx0vT6lzQSh350/vzqBr26Z888je9Cltzqg0SyoiIsmocXw3ddaQrny+cC0j3p/N65OX8LNTB1FeVc2bU5byvxkr+VKXVlx+RG8AJsxbw+3DBxGLRjiiXylPjp1HWUUVTYqiOX4KEdkdKXHspoqiEX75tX04Y3Bnbvz351z+6DgA2jQr4pBe7RkzZxXnPvgRTYuitG9ezFkHdgPgyL068Pf/zWH83NUclsXp292dd6ct5/B+HSjSgEWRgqLEsZs7uHd7/vvdI3jl88V0adOMA7q3IRaNUFZRxXMTFvLEmHmcN7Q7TYuD0sXBvdpTFDXem748q4njhU8X8d0nP+GmrwyoLfmISGHQn4IFoCQW5av7d2Vor3a105E0KYpy/sHdefHawzn/4O61xzYviXFgj7aM+iJ77Rzuzp/fnQnAiPdna30RkQKjxNEIHdGvlMmL17F8/RbKK6t55H9zePj92btsVPk705Yxdcl6hg/uzOK1Zbz46aJdcl0RyQ+qqmqEjujXgd+8No0/vDWdD2auYNbyjUBQvfTbs/ajb8cWO3X9P70zky5tmvKbM/dj6uL1/OW9mXx1/y5EIrYrwheRHFOJoxEa1Lk1bZsV8dhHc3GHERcN4Q/n7c+clRs55Q+juPfNL1i8dscWkhozexXj5q7myiN7UxyL8M2jevPF0g28M23ZLn4KEckVK8TpJ4YMGeLjxo3LdRh57cVPF7F6UznnHtSd4ljw98Oy9WXc/PxEXpu0FDM4rE8HLjy0B18etEfa173o4TFMXLiW9390LE2Lo1RUVXP0b96lS5umPH3Vodl6HBHZBcxsvLsPqe84lTgaqdP268yFh/asTRoAHVs24S/fGMK7Pzia7xzbj7mrNnLlY+NrG7rr8+60ZYz8YjmXHNazthdXUTTCZYf3YsycVYyfuzorzyIiDUuJQ7bTs0Nzvn/CXrx9/dGcvl9nfv3qVO56dSruzrqyCp4eO59fvzqVhXHror8zdRlXPjae/nu05KJhPbe53jkHdaN10yIeGjWrgZ9ERLJBjeOSUlE0wj3nDKZ5SYw/vTuTD2asYMqS9bXdax9+fzZXHNGbfp1a8INnPqX/Hq147LKhtGxStM11mpfEOP/g7vxl5EzmrdxE9/bNcvE4IrKLKHFInaIR4xdf/RJtmhXx7wkLOX9od4YP7kxpyxJ+89o07ntnBgCDu7XhkUuH0rppUdLrXDysJw+NmsXDH8zm1tMHNeQjiMgupsZx2Skfz1vNW1OW8c2jem9X0kh03VOf8OqkJXx443EpE4yI5I4ax6VB7N+9LT84ce96kwbAZUf0YlN5FU+MmdcAkdXtnanLOO2P7/PxPDXYi2RKiUMazKDOrRnWpz1//2AOFTla+9zdefj92Vz2yFg+X7iWbz0+gZUbtuQkFpHdlRKHNKgrjujNknVlXPbIOP45eh7zV21izOxV3PXqVL72pw/47WvTqK7OTvVpZVU1Nz0/kdtfmszxAzrx7FWHsnpTOd958mOqsnRPkUKkxnFpUEftVcq3j+7Dfz5ZxE+++Lx2fzRi9C1twX3vzGDWig387uzBO7xeyJpN5cxZuYnB3dpss/93b3zBP0bP46qj+vDDE/cmEjHuOONL3PDsZ9z9+jR+eFL/nXo2kcZCiUMaVCRi/PCk/txw4t5MX7aB96evYI/WTTisbwdaNYkx4v3Z3PHyFJauG80Fh3RnzOzVjJ69krLyKnq0b07PDs3p0KKYaneqqqFd8yKO2bsjfTu2oKLKeeyjufzhrems3VzBL7+2D+cNDWYG/mjWSv48cibnDOnGj0/emiDOGtKNCfNW86d3Z3JEv1IO7dM+V78akd2GelVJ3nnl88V876lPKK+spmVJjIN6taN10yLmrNzInBUbWbO5gqgZkYjVjinp3i4YGzJv1SaO6NcBd/jfzBXcd/4BHNanAyf//j1KiqK8dO3hNC/Z9u+lsooqjrjrHQbu2YpHLh2aUazuzsgvltO+eQn7dG29a34BCSqqqnlq7HyOH9CJPVo3yco9RCD9XlUqcUjeOWWfPdmnS2vWbKpgYOdWROuYVXfx2s28NWUZb01ZyrqySm4bPoij9yqlrKKaC0aM5ntPfsI+XVuzbP0W/vWtYdslDQjWLrnwkB7c/cYXTF+6nn6dWqYV54R5q/nlK1MYO2c1LZvEePnaI7IyuPGVzxdz0/MTufv1afz6//bNaO4wkWxQiUMK1tpNFZz9lw+ZtnQ9Pzxpb759dN+Ux67aWM6hv3yLrx3QhV9+bd9t3nN3Ji1ax38+WcjCNZvZVF7Fmk0VfDJ/DR1alHDlkb247+0ZdG/fjGevGpZR28zk8LrXHtePFkmSGsA3RoxmxrINtG9RzMSF6/j6wd25+dSBWjNedjmVOKTRa92siMcvP5iRXyznq/t3qfPYds2L+doBXXhuwkJuOLE/7ZoXU13t/HPMPB7/aC5Tl6ynOBqhe/tmNCuO0rQoyveP34vLj+hF85IYvTq04IpHx3HHy5O544x9gCDhAJglLzHNWLaBC0aMZtXGcj6avYpHLjmINs2Ktzlm0ZrNvD9jBdce249rjunL3W9M4y8jZ1Htvl2CE2koShxS0EpblnDmgV3TOvbSw3rxxJj5/OOjuVxyeC+uf/oTXpu0lH26tObnwwdx+n5daN0s+UDHEwZ24soje/Pge7Moq6hmydoyPluwhuJYlMuP6MUFh/TYpkSxcM1mvjFiNBGD204fxJ0vT+HcBz/iscsOprRlSe1x//54Ie5w5gFdKY5FuPHkARjGAyNncmz/TpwwsNPO/YKSqK52NlVUpSwBiaiqSiTOhQ+PYfKidbRvXsz0Zev56VcGculhPVOWGuJVVFVzwUOjGT93Nf33bMk+XdqwYPUmRk1fQeumRQwf3JmOLUto3bSIv30wh+UbtvDUlYcysHMr3p++giseHccerZvw1DcPoWPLJrg7x949ko4tS3jqm1vXMimvrOaM+z9g6boyXv3ekdskmrq4O796dSrL123hhpP2Zs/WTbc7ZvrS9Vz/zKfMXrGRf3/7sJ1eDVJ2L+lWVSlxiMQZ+cVyLnp4DK2bFnH/+QdweL8OGZ1fVe1UVldTEtva/vDp/DXc984MRk1fTllF0AusWXGURy8dypCe7WqPGzdnFRc+PIYe7Zvz1DcP4Ysl6znzgQ/5zZn7ctaQbtvcZ/rS9Zz6x/c5rG8HRlw0JK3E9thHc7n5+YmYQdOiKN89rh+XHNYLsyAZ/WP0XH77+he1JY22zYp4/urD0ppORgpD3iUOMzsJ+D0QBR5y918lvF8CPAocCKwEznH3OWZ2AvAroBgoB25w97frupcSh+wod+dfExYytGe7rPSQ2lJZxdrNFTQtiib9Qn7vi+Vc+vexHNijLZ3bNOW1SUsY+9Pjk/YG+9sHs7ntxcnccOLeXH1M6oZ/gPFzV3Pugx9yeN8O3Hr6IH7+0mTenLL9cr4nDurEHWfsU9v+cvyAjjxwwYFpJSbZ/eVV4jCzKPAFcAKwABgLnOfuk+OO+Tawr7tfZWbnAl9193PMbH9gqbsvMrMvAa+5e50tnUocsjv7zycL+e6TnwBw5oFd+e1Z+yU9rrra+f7Tn/CfTxZtkzzGzVnFLf+ZRCQCp+7bmWF92nPFo+MoiUV58ZrDa9tp3pm2jAlzV1McjVAci9C3YwuO7d+xNkk8NGoWd7w8hSuO6MWAPVuxckM5ZnD+wd1pVpx5+8eGLZU0L44qCeWxfOtVNRSY4e6zAMzsSWA4MDnumOHAreHrZ4H7zMzc/eO4YyYBTc2sxN01M50UpOGDu7BiQzm//u/U2pHvyUQixt1n7YcBv3ltGuWV1WyuqOKvo2bRuXVTOrQs4Vf/nQpAk6IIz31r6DaN+8fs3ZFj9u6Y8vqXHd6Lzxas5a+jZm+z/4kx8/jjeQcwsHOrtJ7H3Xl89DzueGkyX92/C7/6v92rN9g705bRuXVT9t4jvfE9jUFDlTjOBE5y98vD7W8AB7v7NXHHTAyPWRBuzwyPWZFwnavc/fgk97gSuBKge/fuB86dOzebjySSdZvLq2rXbq9LVbVzw7Of8tyEhUBQIvjJKQNoURJj3spNvDJxMQP3bMWRe5VmHENVtTN50TpaNInRvkUxny9Yy/ef+oQ1myq48ZT+XHhozzoHaK7ZVM6P/vUZr01aSrd2TZm/ajO//r99OOegbROiu7O5oooNZZVUuVMSi1Ici9CsKEqkjutn25jZqzjnwQ9p1aSIf31rWMF3Fsi3qqqdThxmNgh4Afiyu8+s636qqpLGpqo6mC5+wJ6tMm7Qz9TKDVu44dnPeHvqMvqUNuc7x/Xj1H071yaQjVsq+WDGCt79YjmvT1rC2s0V/Oik/lw8rCcX/20sY+as4rlvDeNLXVrzvxkr+MV/pzBl8fqkMxQXxyL0bN+MXh2ac8zeHTm3jhJYOqqrnbenLmNY3/b1VretL6vg5N+PwixI4iWxKP++ehgdWxbutC/5ljgOBW519xPD7RsB3P2Xcce8Fh7zoZnFgCVAqbu7mXUF3gYucfcP6rufEodIdrk7/524hN+/OZ1pS9fTtW1TmhRFWbFhC2s2VQDQvDjKYX07cM2xfdm3azBT8coNWzj1j+8TjRiDOrfitUlL6dq2Kaft15lWTYpo2SRGNJyDrKyiipUby5m1fCPTl61n7spN3PSVAVx+RO/aGP46ahZvTl7GvecOpnOb7bsXJ3pizDxufO5zhvVpz8MXH1Tn6PsfPPMpz01YwDNXHUpRNMI5f/mIPh2b89SVhybtrFAI8i1xxAgax48DFhI0jp/v7pPijrka2Ceucfxr7n62mbUBRgK3uftz6dxPiUOkYVRXO69OWsIz4+bTtDhK++YllLYsYUjPtgzp0Y7i2PZL/nw8bzVn/+VDiqIRrj6mL5cd3qve6VOqqp1rn5jAK58v4e6z9uP0wZ25+fmJPDl2PhGDLm2b8sQVh9C1beqecBu2VHL0b96lJBZh4ZrNfHlgJ/709QOIRbeP8dWJi7nq8Qlcc0xffnDi3kCwauTlj47jsL4deOjCIUmfbdm6Mm75zySmL1vPzacO5Og62pDin23purK0El+25VXiADCzU4B7CbrjPuzud5rZ7cA4d3/BzJoAjwH7A6uAc919lpndBNwITI+73Jfdffu+hCElDpH8NnXJOto1K6Zjq/SrfbZUVnHp38fy0axVfKlLaz6dv4arj+nDCQP34MIRo2nVtIgnrjiE5iUxpi5ex9rNFZwwsFNtYrj79Wn88e0Z/Pvbw/h0/hpufXEy/3dAV35z5r617SjV1c4TY+fxi5en0Lu0Bc99exhFcYnlqbHz+NG/Pmf44M7cc/bg2vPcnX9/vJDbXpxMWUUVnVo1Yd6qTXxl3z356SkD2LN1k216k1VVOzOWbeDfHy/k+Y8XsmRdGcfsXcrNpw6kd2nu2lHyLnE0JCUOkcK0YUsl5//1IyYvWscvvroPZx8UDIz8fMFaLhgxmk3llVRUbf1OG9anPfedfwBbKqs45rfvcsLAPfjjefsD8Ps3p3PPm1/QuXUTTtlnTw7r14G/jJzJR7NWcWjv9tx99n5JSwF/encGd706jYuH9eSnXxnAa5OW8PcP5jBu7mqG9GjLXWfuS5e2TXlw5Cz++M4MyiuraVoUpbRlCc2Ko6zaWM7KjeVUVTvRiHHUXqUM2LMlj/xvbpAcD+/Ft4/qm3J6m3jV1c5D788Ku1Lv/PQzShxKHCIFaVN5JcvXb6FH++bb7J+yeB1PjplHt3bN6L9HK+av3sTPXphEx5Yl9C5twUezVvLWdUfRLVy7xd155fMlPDdhAe9NX05FldOyJMZPvzKAcw7qlnK8ibtzx8tTGPH+bFo3LWLt5gq6tWvKlUf05vyDe2zTy2zuyo28OnEJy9dvYfmGLWzcUllbnde5TVNOGNipdsqYZevLuOvVaTw7fgEtSmJceGgPLju8F+1bJJ9Spryymuuf+ZQXP10EwIWH9uAnpwzYqVmTlTiUOEQavU/mr+Gqx8azZF0Z3zyqNzeePCDpcWs3VfDhrJXs370NndKoPquudm5/aTLzV23i64d056i9OtbZLTkTUxav4753ZvDK54spjkbov0dLurdvTs/2zfhSl9Yc2KMtzYqjfOvxCYz8Yjk/PGlvVm0o56H3Z9N/j5bcd/7+9O24Y2NOlDiUOESE4C/5f41fyIWH9titekPNWLaBxz+ay4xlG5i7aiMLV2+mpsdyyyYxNm6p5Jdf2zom5p2py7j+mU/Zv1sbRlx80A7dU4lDiUNECkhZRRWTFq1l3JzVTF68jtP368xxA7Zt11i2rgwgo04H8fJtyhEREdkJTYqiHNijHQf2aJfymB1NGJnaviOyiIhIHZQ4REQkI0ocIiKSESUOERHJiBKHiIhkRIlDREQyosQhIiIZUeIQEZGMKHGIiEhGlDhERCQjShwiIpIRJQ4REcmIEoeIiGSkIKdVN7PlwNyduEQHYMUuCmd30RifGRrnc+uZG49Mn7uHu5fWd1BBJo6dZWbj0pmTvpA0xmeGxvnceubGI1vPraoqERHJiBKHiIhkRIkjuQdzHUAONMZnhsb53HrmxiMrz602DhERyYhKHCIikhEljjhmdpKZTTOzGWb241zHkw1m1s3M3jGzyWY2ycy+G+5vZ2ZvmNn08N+2uY41G8wsamYfm9lL4XYvMxsdfuZPmVlxrmPclcysjZk9a2ZTzWyKmR3aGD5rM/t++P97opk9YWZNCvGzNrOHzWyZmU2M25f087XAH8Ln/8zMDtjR+ypxhMwsCtwPnAwMBM4zs4G5jSorKoHr3X0gcAhwdficPwbecvd+wFvhdiH6LjAlbvvXwD3u3hdYDVyWk6iy5/fAq+7eH9iP4NkL+rM2sy7Ad4Ah7v4lIAqcS2F+1n8HTkrYl+rzPRnoF/5cCfx5R2+qxLHVUGCGu89y93LgSWB4jmPa5dx9sbtPCF+vJ/gi6ULwrI+Ehz0CnJGbCLPHzLoCXwEeCrcNOBZ4NjykoJ7bzFoDRwIjANy93N3X0Ag+ayAGNDWzGNAMWEwBftbu/h6wKmF3qs93OPCoBz4C2pjZnjtyXyWOrboA8+O2F4T7CpaZ9QT2B0YDndx9cfjWEqBTjsLKpnuBHwLV4XZ7YI27V4bbhfaZ9wKWA38Lq+ceMrPmFPhn7e4Lgd8C8wgSxlpgPIX9WcdL9fnusu84JY5GysxaAP8Cvufu6+Lf86CrXUF1tzOzU4Fl7j4+17E0oBhwAPBnd98f2EhCtVSBftZtCf667gV0BpqzfXVOo5Ctz1eJY6uFQLe47a7hvoJjZkUESeMf7v5cuHtpTbE1/HdZruLLksOA081sDkE15LEE9f9twuoMKLzPfAGwwN1Hh9vPEiSSQv+sjwdmu/tyd68AniP4/Av5s46X6vPdZd9xShxbjQX6hT0vigka017IcUy7XFivPwKY4u6/i3vrBeCi8PVFwH8aOrZscvcb3b2ru/ck+GzfdvevA+8AZ4aHFdRzu/sSYL6Z7R3uOg6YTIF/1gRVVIeYWbPw/3vNcxfsZ50g1ef7AnBh2LvqEGBtXJVWRjQAMI6ZnUJQDx4FHnb3O3Mc0i5nZocDo4DP2VrX/xOCdo6nge4EMwuf7e6JjW4FwcyOBn7g7qeaWW+CEkg74GPgAnffksv4diUzG0zQGaAYmAVcQvAHY0F/1mZ2G3AOQS/Cj4HLCerzC+qzNrMngKMJZsFdCvwMeJ4kn2+YRO8jqLbbBFzi7uN26L5KHCIikglVVYmISEaUOEREJCNKHCIikhElDhERyYgSh4iIZESJQ2QHmVknM3vPzNab2d25jgfAzOaY2fG5jkMKW6z+Q0QKi5mNAS4g6OP/rLvv6PTSVwIrgFaufu3SiKjEIY1KON1KD2A6cCAwYScu1wOYrKQhjY0ShzQ2X2Lrl/0Q6kkcZjbMzMaa2drw32Hh/r8TTOfwQzPbkKx6yMxKzOy3ZjbPzJaa2QNm1jR872gzW2BmPzGzFWEV09fjzm1tZo+a2XIzm2tmN5lZJO79K8KFmdZbsChXfKlpcLhQz9pwwaIm4TkdzOwlM1tjZqvMbFT8NUXSpaoqaRTM7BLgHoKpNyJmtgZoAWw2s18A+7v77IRz2gEvEywK9ARwFvCymfV194uDGRxY4O43pbjtr4A+wGCgAvgncAtwY/j+HgRTRXQhWFTrFTMb5+7TgD8CrYHeBNO/v04wRfgIMzsLuJVgnYVx4T0q4u57NsG0EmXAB8DFwAPA9QQTH5aGxx1Cgc2MKw1Df21Io+Duf3P3NgTrMhwC7AtMJGifaJOYNEJfAaa7+2PuXunuTwBTgdPqu184L9CVwPfdfVW4aNYvCCZYjHezu29x95EESerscDXKc4Eb3X29u88B7ga+EZ5zOXCXu48NF+WZ4e5z4675B3dfFM4/9SJB4oIguewJ9HD3CncfpWo22RFKHFLwwjWY15jZWmAY8C4wDdgbWG1m30txameCSeLizSW9xW9KCVaeGx/eew3wKlv/2gdY7e4bE67dmaAUUpRw7/j7dgNm1nHvJXGvNxGUrAB+A8wAXjezWWZWUEvGSsNR4pCCF/7F3wb4JvBQ+PpV4LSwtHFvilMXETSAx+tOemsYrAA2A4PCe7Rx99bu3iLumLbhinzx114UnluRcO/4+84nqJ7KSFh6ud7dewOnA9eZ2XGZXkdEiUMak/heVPsTVFvV5RVgLzM738xiZnYOMBB4qb4buXs18FfgHjPrCGBmXczsxIRDbzOzYjM7AjgVeMbdqwimxb7TzFqaWQ/gOuDx8JyHgB+Y2YHh2gp9w2PqZGanhscawXKqVWydWl8kbUoc0pgcCEwws/ZAlbuvrutgd19J8GV+PbCSYL3yU919RZr3+xFB1dBHZrYOeJOgeqzGEmA1QSnjH8BV7j41fO9agqVeZwHvEzSsPxzG9QxwZ7hvPcH6C+3SiKdfGMMG4EPgT+7+TprPIlJL63GI5EC4mNTj7t4117GIZEolDhERyYgSh4iIZERVVSIikhGVOEREJCNKHCIikhElDhERyYgSh4iIZESJQ0REMqLEISIiGfl/KIrcJFt+J08AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mae'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 6, 6\n",
    "\n",
    "# fig.clear()\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "ax1.plot(bt_history.epoch, bt_history.history['loss'])\n",
    "ax1.set_title('Convergence Error')\n",
    "\n",
    "if bt_model.loss == 'mae':\n",
    "    ax1.set_ylabel('Mean Absolute Error (MAE)',fontsize=12)\n",
    "# just in case you decided to change the model loss calculation\n",
    "else:\n",
    "    ax1.set_ylabel('Model Loss',fontsize=12)\n",
    "ax1.set_xlabel('# of epochs',fontsize=12)\n",
    "plt.show()\n",
    "bt_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Training History: 0.024224783271648665\n",
      "Scikit: 0.014191409959949499\n",
      "Mean from Prediction: 0.0142\n",
      "dict_keys(['loss'])\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "# From History though: \n",
    "print(\"From Training History: \" + str(np.mean(bt_history.history['loss'])) )\n",
    "\n",
    "a=bt_model.predict(training_inputs[:-pred_range])\n",
    "\n",
    "print( \"Scikit: \" + str(sklearn.metrics.mean_absolute_error(training_outputs, bt_model.predict(training_inputs[:-pred_range]))))\n",
    "\n",
    "print('Mean from Prediction: %.4f'%np.mean(np.abs((bt_model.predict(training_inputs[:-pred_range]))-\\\n",
    "            (training_outputs))))\n",
    "\n",
    "# print('MAE: %.4f'%np.mean(np.abs((bt_model.predict(LSTM_test_inputs[:-pred_range]))-\\\n",
    "#             (LSTM_training_outputs))))\n",
    "print(bt_history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"graph_pred_training_set\"></a>Graph Predicted Values with Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kstruga/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import sklearn.metrics\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "mae_test_error =sklearn.metrics.mean_absolute_error(training_outputs, bt_model.predict(training_inputs[:-pred_range]))\n",
    "\n",
    "# Create a trace\n",
    "real_price = go.Scatter(\n",
    "    x = model_data[model_data['date']< split_date]['date'][window_len:].astype(datetime.datetime),\n",
    "    y = training_set['btc_close'][window_len:],\n",
    "    name = \"Cmimi i Vertete\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgba(152, 0, 0, .8)'\n",
    "    )\n",
    ")\n",
    "\n",
    "predicted_price = go.Scatter(\n",
    "    x = model_data[model_data['date']< split_date]['date'][window_len:].astype(datetime.datetime),\n",
    "    y = ((np.transpose(bt_model.predict(training_inputs))))[0], \n",
    "    name = \"Cmimi i Parashikuar\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = \"#82E0AA\"\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Parashikimi ne Bashkesine e Trajnimit, MAE: %.4f'%+ mae_test_error,\n",
    "              yaxis = dict(title = 'Bitcoin Price (USD)'),\n",
    "            )\n",
    "\n",
    "data = [real_price,predicted_price]\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='bitcoin-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_data[model_data['date']>=split_date]\n",
    "len(test_outputs)\n",
    "len(test_set['btc_close'][window_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_set['btc_close'].values[window_len:].min()\n",
    "a=test_set['btc_close'].values[window_len:]\n",
    "\n",
    "real_values = []\n",
    "\n",
    "min_value= a.min()\n",
    "max_value= a.max()\n",
    "\n",
    "for x in np.nditer(a):\n",
    "    k= (x - min_value)/(max_value - min_value)\n",
    "    real_values.append(k)\n",
    "\n",
    "# print(real_values)\n",
    "# for i in range(test_set['btc_close'].values[window_len:]):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "test_data_used = test_set['btc_close'].values[window_len:]\n",
    "\n",
    "exchange_data_reverse = pd.read_csv('exchange_data_reverse.csv').iloc[:, 1:]\n",
    "\n",
    "real_training_set, real_test_set = exchange_data_reverse[exchange_data_reverse['date']<split_date], exchange_data_reverse[exchange_data_reverse['date']>=split_date]\n",
    "\n",
    "# print(real_test_set['btc_close'].values[window_len:])\n",
    "\n",
    "real_test_data_used = real_test_set.values[window_len:]\n",
    "\n",
    "print(len(real_test_data_used))\n",
    "print(len(test_data_used))\n",
    "# for i, (test_data_used, real_test_data_used) in enumerate(zip(eth_pred_prices, bt_pred_prices)):\n",
    "#     # Only adding lines to the legend once\n",
    "#     if i<5:\n",
    "\n",
    "# test_data_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40973186, 0.4143368 , 0.41282147, 0.409403  , 0.4215629 ,\n",
       "       0.4170653 , 0.41710758, 0.42189765, 0.41945753, 0.43406254,\n",
       "       0.4326306 , 0.4337323 , 0.43074226, 0.4308077 , 0.42968807,\n",
       "       0.42823905, 0.42875284, 0.4236344 , 0.41772223, 0.42127118,\n",
       "       0.4097243 , 0.41112733, 0.40110224, 0.39978623, 0.39615524,\n",
       "       0.39262345, 0.38463867, 0.3685216 , 0.3797532 , 0.38190576],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt_model.predict(test_inputs)[69]\n",
    "# 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"graph_pred_test_set\"></a>Graph Predicted Values with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kstruga/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import sklearn.metrics\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# llogaritja errorit me scikit\n",
    "# mae_test_error =sklearn.metrics.mean_absolute_error(testing_outputs, bt_model.predict(test_inputs[:-pred_range]))\n",
    "# ne menyre alternative mund ta llogarsim edhe vete:\n",
    "mae_test_error=np.mean(np.abs((np.transpose(bt_model.predict(test_inputs)))-(test_set['btc_close'].values[window_len:])))\n",
    "\n",
    "# Create a trace\n",
    "real_price = go.Scatter(\n",
    "    x = model_data[model_data['date']>=split_date]['date'][window_len:].astype(datetime.datetime),\n",
    "    y = test_set['btc_close'][window_len:],\n",
    "    name = \"Cmimi i Vertete\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgba(152, 0, 0, .8)'\n",
    "    )\n",
    ")\n",
    "\n",
    "predicted_price = go.Scatter(\n",
    "    x = model_data[model_data['date']>= split_date]['date'][window_len:].astype(datetime.datetime),\n",
    "    y = ((np.transpose(bt_model.predict(test_inputs))))[0], \n",
    "    name = \"Cmimi Parashikuar\",\n",
    "    marker = dict(\n",
    "        size = 5,\n",
    "        color = \"#82E0AA\"\n",
    "    ),\n",
    "    mode = 'lines+markers',\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Parashikimi me te dhenat test, MAE: %.4f'%+ mae_test_error,\n",
    "              yaxis = dict(title = 'Cmimi Bitcoin '))\n",
    "\n",
    "data = [real_price, predicted_price]\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='bitcoin-prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 0s 905us/step\n",
      "0.08189311119045799\n",
      "783/783 [==============================] - 0s 81us/step\n",
      "0.014191409708553804\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    EVALUATING ON TEST SET; MAE: 0.0301\n",
    "\"\"\"\n",
    "print(bt_model.evaluate(test_inputs[:-pred_range], testing_outputs, batch_size=window_len))\n",
    "print(bt_model.evaluate(training_inputs[:-pred_range], training_outputs, batch_size=200))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30)                0         \n",
      "=================================================================\n",
      "Total params: 47,830\n",
      "Trainable params: 47,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-0.09202094, -0.04477758,  0.08959167, ...,  0.01537165,\n",
       "         -0.073314  , -0.06417377],\n",
       "        [ 0.00505547,  0.02361651, -0.06848758, ..., -0.08808359,\n",
       "          0.03462484, -0.13535029],\n",
       "        [-0.01526635, -0.01265426,  0.05488202, ..., -0.10913139,\n",
       "          0.02069129, -0.00211093],\n",
       "        ...,\n",
       "        [-0.03327118, -0.03761009,  0.06236958, ..., -0.06445208,\n",
       "         -0.08988712, -0.16512781],\n",
       "        [-0.04512416, -0.13078038,  0.00329403, ...,  0.1227261 ,\n",
       "         -0.06649701, -0.04742682],\n",
       "        [ 0.01225006, -0.01095224,  0.02674607, ...,  0.07026706,\n",
       "         -0.13056058,  0.04936855]], dtype=float32),\n",
       " array([[-0.1763637 ,  0.10007595,  0.10380053, ...,  0.01303361,\n",
       "         -0.07163014, -0.06301135],\n",
       "        [-0.04113827,  0.02699297,  0.05498387, ..., -0.0218554 ,\n",
       "          0.02485342, -0.02576483],\n",
       "        [ 0.12865253, -0.08270983,  0.00687388, ...,  0.00450752,\n",
       "         -0.00676276,  0.06027142],\n",
       "        ...,\n",
       "        [-0.09419976,  0.03986738,  0.06676518, ..., -0.05865773,\n",
       "         -0.03787048, -0.05300481],\n",
       "        [-0.10817564,  0.0040993 ,  0.017757  , ..., -0.09409738,\n",
       "         -0.12529019, -0.06514362],\n",
       "        [-0.03333069, -0.0035675 ,  0.00794812, ...,  0.00627174,\n",
       "         -0.00592977,  0.06906907]], dtype=float32),\n",
       " array([-1.19253255e-01, -5.37407063e-02, -9.87740383e-02, -6.42801672e-02,\n",
       "        -1.58171445e-01, -7.17669427e-02, -1.14156090e-01, -6.26365095e-02,\n",
       "        -9.54164043e-02, -1.32852867e-01, -1.20966516e-01, -1.46256611e-01,\n",
       "         1.65480599e-02, -7.59986341e-02, -6.24151640e-02, -5.70677668e-02,\n",
       "        -5.18250652e-02, -7.56646395e-02, -3.07326242e-02, -1.18145823e-01,\n",
       "        -1.27287745e-01, -1.27232209e-01, -4.56246585e-02, -1.17281228e-01,\n",
       "        -6.89259991e-02, -2.87306421e-02, -6.39160275e-02, -5.35222329e-02,\n",
       "        -5.39416000e-02, -7.32035264e-02, -9.62418467e-02, -5.87517880e-02,\n",
       "        -8.48211348e-02, -1.06188115e-02, -7.40326121e-02, -3.06658149e-02,\n",
       "        -8.02136436e-02, -1.72259659e-01, -5.70271611e-02, -7.17503577e-02,\n",
       "        -6.71969056e-02, -5.91047704e-02, -2.90589463e-02, -6.53961524e-02,\n",
       "        -6.91271946e-02, -9.86136869e-02, -1.30905122e-01, -4.80334871e-02,\n",
       "        -1.89451128e-02, -3.87767032e-02, -5.92483878e-02, -6.97644353e-02,\n",
       "        -8.49917904e-02, -4.27013189e-02, -8.02202076e-02, -5.04716188e-02,\n",
       "        -7.24181682e-02, -7.64368400e-02, -1.55679863e-02,  1.27254175e-02,\n",
       "        -5.75509481e-02, -9.83557254e-02, -6.45923167e-02, -1.30977750e-01,\n",
       "        -3.72393578e-02, -7.68044665e-02, -9.57628712e-02, -8.29672590e-02,\n",
       "        -7.32564181e-02, -3.40654254e-02, -8.16413239e-02, -7.37505406e-02,\n",
       "        -8.84788707e-02, -8.07289779e-02, -8.95675942e-02, -1.09579220e-01,\n",
       "        -8.04640949e-02, -7.40440190e-02, -1.10645927e-01, -1.29377872e-01,\n",
       "        -8.60843584e-02, -5.31591922e-02, -1.45242929e-01, -8.47774595e-02,\n",
       "        -1.82517439e-01, -4.91147563e-02, -6.95427358e-02, -7.50678629e-02,\n",
       "        -5.48229292e-02, -8.48138854e-02, -5.51369973e-02, -4.47566062e-02,\n",
       "        -1.17160775e-01, -1.06884703e-01, -1.02310106e-01, -7.97366872e-02,\n",
       "        -7.60712922e-02, -2.38732658e-02, -8.23991001e-02, -1.23684727e-01,\n",
       "         9.24163043e-01,  9.52675879e-01,  9.01033282e-01,  1.01855814e+00,\n",
       "         8.87280941e-01,  9.40981567e-01,  8.81572366e-01,  9.44868088e-01,\n",
       "         9.25782382e-01,  9.47092354e-01,  8.82989645e-01,  8.70219469e-01,\n",
       "         1.04506612e+00,  8.97447407e-01,  9.91460085e-01,  8.60775173e-01,\n",
       "         9.68227565e-01,  9.23152506e-01,  9.60241735e-01,  8.79270613e-01,\n",
       "         8.98384631e-01,  8.93996179e-01,  9.93423939e-01,  8.83262694e-01,\n",
       "         9.07623887e-01,  9.87871051e-01,  9.32741582e-01,  9.93644238e-01,\n",
       "         9.54761147e-01,  9.22996521e-01,  8.90985608e-01,  9.68703866e-01,\n",
       "         9.16373253e-01,  8.99446607e-01,  9.36048090e-01,  9.93390977e-01,\n",
       "         8.97796273e-01,  9.11482990e-01,  1.00854921e+00,  9.39552486e-01,\n",
       "         9.51867402e-01,  9.46916401e-01,  9.40611541e-01,  9.05504286e-01,\n",
       "         9.00082886e-01,  9.55085158e-01,  8.96202505e-01,  9.45593774e-01,\n",
       "         9.88609314e-01,  9.73774731e-01,  9.32857692e-01,  9.65615213e-01,\n",
       "         8.64619434e-01,  1.00061369e+00,  9.26287830e-01,  9.41473305e-01,\n",
       "         9.30741847e-01,  9.21691120e-01,  1.00684083e+00,  1.00307524e+00,\n",
       "         9.29546595e-01,  8.96335781e-01,  9.32279825e-01,  9.20410991e-01,\n",
       "         9.03811097e-01,  9.22733366e-01,  9.18374717e-01,  9.48591471e-01,\n",
       "         9.64788258e-01,  8.88073564e-01,  1.04861021e+00,  9.58037019e-01,\n",
       "         8.99242759e-01,  9.21229541e-01,  8.88655066e-01,  9.21683252e-01,\n",
       "         1.08789372e+00,  9.01918471e-01,  9.36619997e-01,  9.24339354e-01,\n",
       "         9.43916142e-01,  9.80124712e-01,  9.09640670e-01,  9.40639675e-01,\n",
       "         8.77063632e-01,  9.71095204e-01,  9.40273583e-01,  9.26163375e-01,\n",
       "         9.51463521e-01,  9.26022649e-01,  9.34462130e-01,  9.46645677e-01,\n",
       "         8.92452896e-01,  9.04427230e-01,  9.01710868e-01,  9.51200306e-01,\n",
       "         8.89911473e-01,  1.00457478e+00,  9.21257019e-01,  9.27040994e-01,\n",
       "        -3.20461276e-03,  6.39028288e-03,  3.26414444e-02, -8.75286292e-03,\n",
       "        -2.17513312e-02,  3.21689136e-02,  5.82141988e-03,  2.51263399e-02,\n",
       "        -2.78383382e-02,  2.61336733e-02,  2.12431755e-02, -3.35990004e-02,\n",
       "         7.62731861e-03, -2.80414093e-02, -1.58054326e-02,  3.57243046e-02,\n",
       "        -1.11697949e-02,  3.70559283e-02, -3.49383405e-03, -3.69073302e-02,\n",
       "         3.74653749e-02, -9.17345006e-03, -1.77022684e-02, -6.41220389e-03,\n",
       "        -6.36457093e-03,  1.33156525e-02, -2.56034173e-02,  1.71413161e-02,\n",
       "        -2.22988380e-03, -8.91646650e-03, -4.59329574e-04, -5.83017804e-03,\n",
       "        -1.11908279e-02,  3.17494594e-03,  4.51822020e-03,  1.12366071e-03,\n",
       "         4.69915457e-02,  3.53192650e-02, -1.00147184e-02,  8.71717278e-03,\n",
       "         1.11513091e-02, -3.88753489e-02, -5.15871756e-02, -1.57683846e-02,\n",
       "         2.87515335e-02,  1.44582782e-02,  9.46593937e-03,  3.92792784e-02,\n",
       "        -5.39786182e-03,  7.34718563e-03, -2.36207095e-04,  3.85999191e-03,\n",
       "         2.95697339e-02,  2.69091744e-02, -1.13582453e-02, -1.44834369e-02,\n",
       "         8.48316029e-03,  1.13009326e-02, -2.42568348e-02, -1.29372152e-02,\n",
       "        -4.22704266e-03,  1.12002669e-02,  2.22013183e-02,  2.62321755e-02,\n",
       "        -1.02063967e-02,  2.93639791e-03,  1.60155036e-02, -2.16898099e-02,\n",
       "         1.63181741e-02, -1.23301533e-03,  1.43487693e-03, -2.10720934e-02,\n",
       "         2.17746906e-02, -1.52063165e-02, -2.46327911e-02,  2.51004603e-02,\n",
       "         2.72052437e-02, -2.84546278e-02, -2.16781889e-04,  1.30215390e-02,\n",
       "         1.42205518e-03, -3.80938724e-02,  6.57382421e-03, -4.76027541e-02,\n",
       "         8.17431975e-03,  3.00583802e-02,  3.56103713e-03, -2.32782774e-02,\n",
       "         7.20205298e-03, -3.48645039e-02,  1.38499876e-02,  9.10692010e-03,\n",
       "        -1.67570226e-02, -2.77048629e-03, -3.31885815e-02,  2.80911243e-03,\n",
       "         4.62181121e-03, -5.81429526e-03,  1.57848769e-03,  2.48603779e-03,\n",
       "        -1.25272900e-01, -6.04868419e-02, -1.01196848e-01, -7.73638934e-02,\n",
       "        -1.42288387e-01, -7.03166276e-02, -1.20773531e-01, -6.70787394e-02,\n",
       "        -9.79748815e-02, -1.28305525e-01, -1.20368920e-01, -1.31734490e-01,\n",
       "        -1.99865513e-02, -8.13813061e-02, -6.55895919e-02, -5.88274784e-02,\n",
       "        -5.86091727e-02, -9.62678492e-02, -2.56282371e-02, -1.19208038e-01,\n",
       "        -1.34114087e-01, -1.27648830e-01, -4.73864563e-02, -1.27325967e-01,\n",
       "        -7.15687796e-02, -3.22895609e-02, -6.45973682e-02, -6.13162480e-02,\n",
       "        -6.31259754e-02, -7.95783997e-02, -9.74699333e-02, -6.16057627e-02,\n",
       "        -7.62882829e-02, -2.34879833e-02, -7.09503070e-02, -4.42700498e-02,\n",
       "        -7.92423263e-02, -1.61683261e-01, -6.22614920e-02, -6.63372651e-02,\n",
       "        -6.97023720e-02, -5.01845926e-02, -2.20018663e-02, -7.20237866e-02,\n",
       "        -7.06552640e-02, -9.92188528e-02, -1.39327079e-01, -4.34303023e-02,\n",
       "        -2.59607714e-02, -4.82393056e-02, -6.14153072e-02, -9.06588510e-02,\n",
       "        -8.62778127e-02, -4.58984599e-02, -8.15333351e-02, -5.34897223e-02,\n",
       "        -8.52769315e-02, -8.01219270e-02, -2.71793585e-02,  1.42099215e-02,\n",
       "        -8.75710621e-02, -1.02551892e-01, -6.89831749e-02, -1.36476964e-01,\n",
       "        -4.59536649e-02, -7.14978650e-02, -9.96229425e-02, -9.31565613e-02,\n",
       "        -7.09202066e-02, -4.00119647e-02, -1.08287781e-01, -7.35116079e-02,\n",
       "        -9.42299515e-02, -9.25743058e-02, -9.53182131e-02, -9.65851098e-02,\n",
       "        -7.97579810e-02, -7.19051063e-02, -1.04366310e-01, -1.20477878e-01,\n",
       "        -8.40091556e-02, -4.54372354e-02, -1.45114988e-01, -7.56712630e-02,\n",
       "        -1.72378555e-01, -5.88006116e-02, -7.66097233e-02, -8.17500651e-02,\n",
       "        -5.76052703e-02, -8.20051879e-02, -5.33731543e-02, -5.12995422e-02,\n",
       "        -1.28329247e-01, -1.09045997e-01, -9.70425382e-02, -8.31617564e-02,\n",
       "        -7.10455924e-02, -2.08843201e-02, -8.26952606e-02, -1.18410386e-01],\n",
       "       dtype=float32),\n",
       " array([[ 0.01230106,  0.05976423,  0.13706963, ...,  0.10712471,\n",
       "          0.00324922,  0.16515622],\n",
       "        [ 0.04058111,  0.05746649,  0.08569571, ...,  0.11331906,\n",
       "          0.12043469,  0.08081584],\n",
       "        [-0.03965283, -0.05971223, -0.09903435, ..., -0.01984033,\n",
       "          0.00579589,  0.00882316],\n",
       "        ...,\n",
       "        [ 0.06085996,  0.06202089,  0.04227069, ...,  0.13910607,\n",
       "          0.18222897,  0.1763385 ],\n",
       "        [ 0.09897398,  0.1135226 ,  0.16677403, ...,  0.01189263,\n",
       "          0.09300365,  0.0904767 ],\n",
       "        [ 0.03015566,  0.01479833, -0.00440876, ..., -0.01436109,\n",
       "          0.18069518,  0.06325957]], dtype=float32),\n",
       " array([0.04287576, 0.04233463, 0.04220861, 0.04348812, 0.0435456 ,\n",
       "        0.04307473, 0.04265796, 0.04433472, 0.04359058, 0.04556897,\n",
       "        0.04389692, 0.04588813, 0.0438212 , 0.04316594, 0.04378996,\n",
       "        0.04734164, 0.0485022 , 0.04634458, 0.04675278, 0.04733827,\n",
       "        0.04737271, 0.04860656, 0.0472875 , 0.04949271, 0.05021678,\n",
       "        0.05013393, 0.04989025, 0.0494053 , 0.05120934, 0.05154744],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bt_model\n",
    "bt_model.summary()\n",
    "bt_model.save('bt_model_dense.h5')\n",
    "bt_model.get_weights()\n",
    "# bt_model.save_weights('bt_model_weights') "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "200ff45abb3b47f3ade09c6ce91695fb": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
