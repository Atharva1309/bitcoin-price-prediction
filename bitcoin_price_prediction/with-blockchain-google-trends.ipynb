{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Imports](#Imports)\n",
    "2. [Setup of Initial Variables](#setup_init_variables)\n",
    "3. [Get Bitcoin Logo](#get_btc_logo)\n",
    "4. [Utility Functions for Data Preprocessing](#util_data_preprocess)\n",
    "5. [Download Data Sets](#download_datasets)\n",
    "    - [Extraction of Blockchain Data](#blockchain_data)\n",
    "    - [Extraction of Macroeconomic Data](#macroecon_data)\n",
    "    - [Extraction of Exchange Data](#exchange_data)<br><br>\n",
    "     - [Vizualize Exchange Data](#price_vizualization)\n",
    "     - [Preprocess Exchange Data](#preprocess_exch_data)\n",
    "     - [Plot Histogram of Daily Price Changes](#hist_daily_price_ch)<br><br>\n",
    "    - [Extraction of Global Currencies Exchange Data](#global_curr_exch_data)\n",
    "    - [Extraction of Sentiment Data](#sentiment_data)<br><br>\n",
    "7. [Merge of All Data Source](#merge_data_source)\n",
    "8. [Min-Max Scaling (a.k.a Normalization)](#normalization)\n",
    "9. [Statistic of Data Prior Training](#statistics)\n",
    "    - [Visualize the Whole Dataset](#viz_dataset)\n",
    "    - [Looking for Trends](#sesonality_differentiation)\n",
    "    - [Pearson Correlation](#pearson_corr)<br><br>\n",
    "10. [ML Pipeline](#ml_pipeline)\n",
    "   - [Split Data (Testing, Training Data Sets)](#split_data)\n",
    "   - [Set Window Length](#win_len_metrics)\n",
    "   - [Fill Training, Test Data](#fill_training_test)\n",
    "   - [Load Model](#load_model)\n",
    "   - [Train Model](#train_model)\n",
    "   - [Graph Predicted Values with Training Set](#graph_pred_training_set)\n",
    "   - [Graph Predicted Values with Test Data](#graph_pred_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptory import Cryptory\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as urllib\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import quandl\n",
    "import time\n",
    "\n",
    "from_date=\"2013-04-28\"\n",
    "to_date=\"2018-10-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# accounts = go.Scatter(\n",
    "#                 x=bch_accounts.Date,\n",
    "#                 y=bch_accounts['btc_Accounts'],\n",
    "#                 name = \"Accounts\",\n",
    "#                 line = dict(color = '#6D13C1'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# high = go.Scatter(\n",
    "#                 x=bitcoin_market_info.Date,\n",
    "#                 y=bitcoin_market_info['btc_High'],\n",
    "#                 name = \"High\",\n",
    "#                 line = dict(color = '#17BECF'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# low =go.Scatter(\n",
    "#                 x=bitcoin_market_info.Date,\n",
    "#                 y=bitcoin_market_info['btc_Low'],\n",
    "#                 name = \"Low\",\n",
    "#                 line = dict(color = '#7F7F7F'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# block_size =go.Scatter(\n",
    "#                 x=avg_block_size.Date,\n",
    "#                 y=avg_block_size['btc_avg_block_size'],\n",
    "#                 name = \"Avg. blockcsize\",\n",
    "#                 line = dict(color = '#D8B1B1'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# txs =go.Scatter(\n",
    "#                 x=txs_data.Date,\n",
    "#                 y=txs_data['btc_transactions'],\n",
    "#                 name = \"Nr of Transactions\",\n",
    "#                 line = dict(color = '#E125E8'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# miner_revenue =go.Scatter(\n",
    "#                 x=bchain_mirev_data.Date,\n",
    "#                 y=bchain_mirev_data['btc_mining_revenue'],\n",
    "#                 name = \"Miners Revenue\",\n",
    "#                 line = dict(color = '#3D0A3F'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# s_and_p =go.Scatter(\n",
    "#                 x=s_and_p_stock.Date,\n",
    "#                 y=s_and_p_stock['sp_close'],\n",
    "#                 name = \"S&P 500 Closing Price\",\n",
    "#                 line = dict(color = '#68340B'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# dow_jones_stock =go.Scatter(\n",
    "#                 x=dow_jones_stock.Date,\n",
    "#                 y=dow_jones_stock['dj_close'],\n",
    "#                 name = \"Dow Jones Closing Price\",\n",
    "#                 line = dict(color = '#00E1FF'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "\n",
    "# data = [low, high, block_size, txs, accounts, s_and_p, dow_jones_stock, miner_revenue]\n",
    "\n",
    "# layout = dict(\n",
    "#     title = \"Manually Set Date Range\",\n",
    "#     xaxis = dict(\n",
    "#         range = ['2014-01-01','2018-09-23'])\n",
    "# )\n",
    "\n",
    "# fig = dict(data=data, layout=layout)\n",
    "# py.iplot(fig, filename = \"Manually Set Range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "\n",
    "Like I said, if you're interested in the theory behind LSTMs, then I'll refer you to [this](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [this](http://blog.echen.me/2017/05/30/exploring-lstms/) and [this](http://www.bioinf.jku.at/publications/older/2604.pdf). Luckily, we don't need to build the network from scratch (or even understand it), there exists packages that include standard implementations of various deep learning algorithms (e.g. [TensorFlow](https://www.tensorflow.org/get_started/get_started), [Keras](https://keras.io/#keras-the-python-deep-learning-library), [PyTorch](http://pytorch.org/), etc.). I'll opt for Keras, as I find it the most intuitive for non-experts. If you're not familiar with Keras, then check out my [previous tutorial](https://dashee87.github.io/data%20science/deep%20learning/python/another-keras-tutorial-for-neural-network-beginners/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>btc_high</th>\n",
       "      <th>btc_close</th>\n",
       "      <th>btc_volume</th>\n",
       "      <th>btc_market_cap</th>\n",
       "      <th>bch_avg_block_size</th>\n",
       "      <th>bch_transactions</th>\n",
       "      <th>bch_mining_revenue</th>\n",
       "      <th>bch_accounts</th>\n",
       "      <th>sp_close</th>\n",
       "      <th>dj_close</th>\n",
       "      <th>google_trends_bitcoin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0.321552</td>\n",
       "      <td>0.331202</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.339832</td>\n",
       "      <td>0.727437</td>\n",
       "      <td>0.523625</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975489</td>\n",
       "      <td>0.982462</td>\n",
       "      <td>0.041897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>0.319950</td>\n",
       "      <td>0.327366</td>\n",
       "      <td>0.162952</td>\n",
       "      <td>0.342786</td>\n",
       "      <td>0.727437</td>\n",
       "      <td>0.523625</td>\n",
       "      <td>0.205355</td>\n",
       "      <td>0.999343</td>\n",
       "      <td>0.995592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>0.321981</td>\n",
       "      <td>0.330136</td>\n",
       "      <td>0.166809</td>\n",
       "      <td>0.344857</td>\n",
       "      <td>0.746110</td>\n",
       "      <td>0.524973</td>\n",
       "      <td>0.238706</td>\n",
       "      <td>0.998520</td>\n",
       "      <td>0.993843</td>\n",
       "      <td>0.995247</td>\n",
       "      <td>0.041897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>0.324067</td>\n",
       "      <td>0.331871</td>\n",
       "      <td>0.167720</td>\n",
       "      <td>0.346238</td>\n",
       "      <td>0.746110</td>\n",
       "      <td>0.524973</td>\n",
       "      <td>0.245823</td>\n",
       "      <td>0.997272</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>0.984533</td>\n",
       "      <td>0.038763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>0.323588</td>\n",
       "      <td>0.333732</td>\n",
       "      <td>0.167775</td>\n",
       "      <td>0.345387</td>\n",
       "      <td>0.584187</td>\n",
       "      <td>0.461143</td>\n",
       "      <td>0.227486</td>\n",
       "      <td>0.996607</td>\n",
       "      <td>0.985894</td>\n",
       "      <td>0.967694</td>\n",
       "      <td>0.037195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>0.323554</td>\n",
       "      <td>0.332510</td>\n",
       "      <td>0.182936</td>\n",
       "      <td>0.347399</td>\n",
       "      <td>0.584187</td>\n",
       "      <td>0.461143</td>\n",
       "      <td>0.236537</td>\n",
       "      <td>0.995442</td>\n",
       "      <td>0.985894</td>\n",
       "      <td>0.967694</td>\n",
       "      <td>0.037195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014-01-07</td>\n",
       "      <td>0.330694</td>\n",
       "      <td>0.334693</td>\n",
       "      <td>0.210234</td>\n",
       "      <td>0.349265</td>\n",
       "      <td>0.765816</td>\n",
       "      <td>0.540268</td>\n",
       "      <td>0.213521</td>\n",
       "      <td>0.994664</td>\n",
       "      <td>0.985894</td>\n",
       "      <td>0.967694</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014-01-08</td>\n",
       "      <td>0.327025</td>\n",
       "      <td>0.336381</td>\n",
       "      <td>0.193135</td>\n",
       "      <td>0.339428</td>\n",
       "      <td>0.765816</td>\n",
       "      <td>0.540268</td>\n",
       "      <td>0.204635</td>\n",
       "      <td>0.993679</td>\n",
       "      <td>0.985911</td>\n",
       "      <td>0.966090</td>\n",
       "      <td>0.045032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014-01-09</td>\n",
       "      <td>0.320677</td>\n",
       "      <td>0.326974</td>\n",
       "      <td>0.186024</td>\n",
       "      <td>0.337123</td>\n",
       "      <td>0.658221</td>\n",
       "      <td>0.549203</td>\n",
       "      <td>0.248511</td>\n",
       "      <td>0.992493</td>\n",
       "      <td>0.979156</td>\n",
       "      <td>0.961319</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.324462</td>\n",
       "      <td>0.198142</td>\n",
       "      <td>0.345137</td>\n",
       "      <td>0.658221</td>\n",
       "      <td>0.549203</td>\n",
       "      <td>0.223961</td>\n",
       "      <td>0.991402</td>\n",
       "      <td>0.987223</td>\n",
       "      <td>0.970654</td>\n",
       "      <td>0.048167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2014-01-11</td>\n",
       "      <td>0.327099</td>\n",
       "      <td>0.332171</td>\n",
       "      <td>0.175117</td>\n",
       "      <td>0.350499</td>\n",
       "      <td>0.546852</td>\n",
       "      <td>0.414158</td>\n",
       "      <td>0.229543</td>\n",
       "      <td>0.990582</td>\n",
       "      <td>0.990428</td>\n",
       "      <td>0.976750</td>\n",
       "      <td>0.045032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2014-01-12</td>\n",
       "      <td>0.329744</td>\n",
       "      <td>0.338135</td>\n",
       "      <td>0.175964</td>\n",
       "      <td>0.351023</td>\n",
       "      <td>0.546852</td>\n",
       "      <td>0.414158</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>0.989905</td>\n",
       "      <td>0.999092</td>\n",
       "      <td>0.992590</td>\n",
       "      <td>0.040330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2014-01-13</td>\n",
       "      <td>0.332180</td>\n",
       "      <td>0.338722</td>\n",
       "      <td>0.189059</td>\n",
       "      <td>0.352037</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>0.603796</td>\n",
       "      <td>0.219989</td>\n",
       "      <td>0.988804</td>\n",
       "      <td>0.999092</td>\n",
       "      <td>0.992590</td>\n",
       "      <td>0.043465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>0.331162</td>\n",
       "      <td>0.339394</td>\n",
       "      <td>0.273893</td>\n",
       "      <td>0.340193</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>0.603796</td>\n",
       "      <td>0.225620</td>\n",
       "      <td>0.987957</td>\n",
       "      <td>0.999092</td>\n",
       "      <td>0.992590</td>\n",
       "      <td>0.052869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>0.317827</td>\n",
       "      <td>0.328251</td>\n",
       "      <td>0.182282</td>\n",
       "      <td>0.334015</td>\n",
       "      <td>0.694020</td>\n",
       "      <td>0.546398</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.986887</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985037</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>0.313762</td>\n",
       "      <td>0.321981</td>\n",
       "      <td>0.185774</td>\n",
       "      <td>0.332536</td>\n",
       "      <td>0.694020</td>\n",
       "      <td>0.546398</td>\n",
       "      <td>0.203242</td>\n",
       "      <td>0.985835</td>\n",
       "      <td>0.980822</td>\n",
       "      <td>0.963107</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2014-01-17</td>\n",
       "      <td>0.310528</td>\n",
       "      <td>0.320571</td>\n",
       "      <td>0.175234</td>\n",
       "      <td>0.327649</td>\n",
       "      <td>0.796096</td>\n",
       "      <td>0.518017</td>\n",
       "      <td>0.195949</td>\n",
       "      <td>0.984613</td>\n",
       "      <td>0.977760</td>\n",
       "      <td>0.949245</td>\n",
       "      <td>0.045032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2014-01-18</td>\n",
       "      <td>0.318378</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>0.163936</td>\n",
       "      <td>0.340056</td>\n",
       "      <td>0.796096</td>\n",
       "      <td>0.518017</td>\n",
       "      <td>0.204166</td>\n",
       "      <td>0.983793</td>\n",
       "      <td>0.964714</td>\n",
       "      <td>0.933109</td>\n",
       "      <td>0.048167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014-01-19</td>\n",
       "      <td>0.318585</td>\n",
       "      <td>0.328122</td>\n",
       "      <td>0.137212</td>\n",
       "      <td>0.341227</td>\n",
       "      <td>0.679602</td>\n",
       "      <td>0.442193</td>\n",
       "      <td>0.207307</td>\n",
       "      <td>0.982960</td>\n",
       "      <td>0.978324</td>\n",
       "      <td>0.941188</td>\n",
       "      <td>0.040330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2014-01-20</td>\n",
       "      <td>0.319460</td>\n",
       "      <td>0.329468</td>\n",
       "      <td>0.134803</td>\n",
       "      <td>0.339737</td>\n",
       "      <td>0.679602</td>\n",
       "      <td>0.442193</td>\n",
       "      <td>0.194971</td>\n",
       "      <td>0.981934</td>\n",
       "      <td>0.978324</td>\n",
       "      <td>0.941188</td>\n",
       "      <td>0.041897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>0.321189</td>\n",
       "      <td>0.327890</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.340024</td>\n",
       "      <td>0.708481</td>\n",
       "      <td>0.502521</td>\n",
       "      <td>0.217838</td>\n",
       "      <td>0.981238</td>\n",
       "      <td>0.978324</td>\n",
       "      <td>0.941188</td>\n",
       "      <td>0.049735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>0.318136</td>\n",
       "      <td>0.328128</td>\n",
       "      <td>0.176527</td>\n",
       "      <td>0.331391</td>\n",
       "      <td>0.708481</td>\n",
       "      <td>0.502521</td>\n",
       "      <td>0.241798</td>\n",
       "      <td>0.980471</td>\n",
       "      <td>0.977651</td>\n",
       "      <td>0.940431</td>\n",
       "      <td>0.054437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2014-01-23</td>\n",
       "      <td>0.309506</td>\n",
       "      <td>0.319561</td>\n",
       "      <td>0.170374</td>\n",
       "      <td>0.329364</td>\n",
       "      <td>0.829894</td>\n",
       "      <td>0.511287</td>\n",
       "      <td>0.202245</td>\n",
       "      <td>0.979443</td>\n",
       "      <td>0.964815</td>\n",
       "      <td>0.927592</td>\n",
       "      <td>0.057572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2014-01-24</td>\n",
       "      <td>0.311270</td>\n",
       "      <td>0.317977</td>\n",
       "      <td>0.161383</td>\n",
       "      <td>0.330123</td>\n",
       "      <td>0.829894</td>\n",
       "      <td>0.511287</td>\n",
       "      <td>0.192920</td>\n",
       "      <td>0.978568</td>\n",
       "      <td>0.963949</td>\n",
       "      <td>0.925161</td>\n",
       "      <td>0.051302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2014-01-25</td>\n",
       "      <td>0.310065</td>\n",
       "      <td>0.318417</td>\n",
       "      <td>0.155686</td>\n",
       "      <td>0.328474</td>\n",
       "      <td>0.507682</td>\n",
       "      <td>0.384416</td>\n",
       "      <td>0.234200</td>\n",
       "      <td>0.977775</td>\n",
       "      <td>0.954898</td>\n",
       "      <td>0.915210</td>\n",
       "      <td>0.052869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2014-01-26</td>\n",
       "      <td>0.313651</td>\n",
       "      <td>0.316925</td>\n",
       "      <td>0.153915</td>\n",
       "      <td>0.324267</td>\n",
       "      <td>0.507682</td>\n",
       "      <td>0.384416</td>\n",
       "      <td>0.217696</td>\n",
       "      <td>0.976860</td>\n",
       "      <td>0.950314</td>\n",
       "      <td>0.920401</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2014-01-27</td>\n",
       "      <td>0.318078</td>\n",
       "      <td>0.313049</td>\n",
       "      <td>0.160760</td>\n",
       "      <td>0.336852</td>\n",
       "      <td>0.778555</td>\n",
       "      <td>0.493607</td>\n",
       "      <td>0.224710</td>\n",
       "      <td>0.976044</td>\n",
       "      <td>0.950314</td>\n",
       "      <td>0.920401</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2014-01-28</td>\n",
       "      <td>0.319136</td>\n",
       "      <td>0.325528</td>\n",
       "      <td>0.178782</td>\n",
       "      <td>0.340478</td>\n",
       "      <td>0.778555</td>\n",
       "      <td>0.493607</td>\n",
       "      <td>0.221641</td>\n",
       "      <td>0.975174</td>\n",
       "      <td>0.950314</td>\n",
       "      <td>0.920401</td>\n",
       "      <td>0.057572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>0.329191</td>\n",
       "      <td>0.328742</td>\n",
       "      <td>0.231588</td>\n",
       "      <td>0.352496</td>\n",
       "      <td>0.895785</td>\n",
       "      <td>0.534508</td>\n",
       "      <td>0.215921</td>\n",
       "      <td>0.974383</td>\n",
       "      <td>0.955672</td>\n",
       "      <td>0.927326</td>\n",
       "      <td>0.076381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>0.361051</td>\n",
       "      <td>0.342390</td>\n",
       "      <td>0.243208</td>\n",
       "      <td>0.384771</td>\n",
       "      <td>0.895785</td>\n",
       "      <td>0.534508</td>\n",
       "      <td>0.211233</td>\n",
       "      <td>0.973446</td>\n",
       "      <td>0.964546</td>\n",
       "      <td>0.925504</td>\n",
       "      <td>0.068544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>2018-09-05</td>\n",
       "      <td>0.031130</td>\n",
       "      <td>0.033176</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.023957</td>\n",
       "      <td>0.083732</td>\n",
       "      <td>0.052955</td>\n",
       "      <td>0.049419</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.043992</td>\n",
       "      <td>0.041535</td>\n",
       "      <td>0.025780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>2018-09-06</td>\n",
       "      <td>0.031450</td>\n",
       "      <td>0.033536</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0.083732</td>\n",
       "      <td>0.052955</td>\n",
       "      <td>0.048978</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.027177</td>\n",
       "      <td>0.031949</td>\n",
       "      <td>0.030096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>2018-09-07</td>\n",
       "      <td>0.031230</td>\n",
       "      <td>0.032838</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>0.089990</td>\n",
       "      <td>0.050244</td>\n",
       "      <td>0.051765</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0.042570</td>\n",
       "      <td>0.048514</td>\n",
       "      <td>0.041965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>2018-09-08</td>\n",
       "      <td>0.034274</td>\n",
       "      <td>0.030710</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.026155</td>\n",
       "      <td>0.089990</td>\n",
       "      <td>0.050244</td>\n",
       "      <td>0.048066</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0.033368</td>\n",
       "      <td>0.040599</td>\n",
       "      <td>0.036570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>2018-09-09</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>0.036605</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.024968</td>\n",
       "      <td>0.075716</td>\n",
       "      <td>0.036229</td>\n",
       "      <td>0.051478</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0.040711</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.024701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>2018-09-10</td>\n",
       "      <td>0.032687</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.075716</td>\n",
       "      <td>0.036229</td>\n",
       "      <td>0.047993</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.040711</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.023622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>2018-09-11</td>\n",
       "      <td>0.030724</td>\n",
       "      <td>0.032039</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.023757</td>\n",
       "      <td>0.081302</td>\n",
       "      <td>0.057495</td>\n",
       "      <td>0.062186</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.040711</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.031175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>2018-09-12</td>\n",
       "      <td>0.032190</td>\n",
       "      <td>0.033331</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.024621</td>\n",
       "      <td>0.081302</td>\n",
       "      <td>0.057495</td>\n",
       "      <td>0.057465</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.072818</td>\n",
       "      <td>0.071978</td>\n",
       "      <td>0.031175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>2018-09-13</td>\n",
       "      <td>0.033124</td>\n",
       "      <td>0.034550</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.025435</td>\n",
       "      <td>0.110552</td>\n",
       "      <td>0.078338</td>\n",
       "      <td>0.062060</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.086612</td>\n",
       "      <td>0.087341</td>\n",
       "      <td>0.035491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>2018-09-14</td>\n",
       "      <td>0.033680</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.110552</td>\n",
       "      <td>0.078338</td>\n",
       "      <td>0.055929</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.085721</td>\n",
       "      <td>0.090928</td>\n",
       "      <td>0.032254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>2018-09-15</td>\n",
       "      <td>0.033941</td>\n",
       "      <td>0.035824</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.025569</td>\n",
       "      <td>0.035210</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.060983</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.081431</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>0.030096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>2018-09-16</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>0.035864</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>0.035210</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.081431</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>0.023622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>0.031682</td>\n",
       "      <td>0.034261</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>0.092124</td>\n",
       "      <td>0.044119</td>\n",
       "      <td>0.050712</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.081431</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>0.023622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>0.031754</td>\n",
       "      <td>0.032948</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.024137</td>\n",
       "      <td>0.092124</td>\n",
       "      <td>0.044119</td>\n",
       "      <td>0.046309</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.081431</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>0.034412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>0.032924</td>\n",
       "      <td>0.034035</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.025102</td>\n",
       "      <td>0.075054</td>\n",
       "      <td>0.049321</td>\n",
       "      <td>0.067627</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.087479</td>\n",
       "      <td>0.091153</td>\n",
       "      <td>0.034412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>0.035343</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.075054</td>\n",
       "      <td>0.049321</td>\n",
       "      <td>0.057637</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.089573</td>\n",
       "      <td>0.096821</td>\n",
       "      <td>0.027938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>0.032397</td>\n",
       "      <td>0.033913</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.024431</td>\n",
       "      <td>0.032692</td>\n",
       "      <td>0.034626</td>\n",
       "      <td>0.048694</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.081582</td>\n",
       "      <td>0.087386</td>\n",
       "      <td>0.027938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>2018-09-22</td>\n",
       "      <td>0.032679</td>\n",
       "      <td>0.034323</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.025064</td>\n",
       "      <td>0.032692</td>\n",
       "      <td>0.034626</td>\n",
       "      <td>0.066565</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.065029</td>\n",
       "      <td>0.077267</td>\n",
       "      <td>0.026598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>0.036061</td>\n",
       "      <td>0.035463</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>0.039344</td>\n",
       "      <td>0.039079</td>\n",
       "      <td>0.065465</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.084518</td>\n",
       "      <td>0.092902</td>\n",
       "      <td>0.026598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>0.035707</td>\n",
       "      <td>0.038089</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.025307</td>\n",
       "      <td>0.039344</td>\n",
       "      <td>0.039079</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.084518</td>\n",
       "      <td>0.092902</td>\n",
       "      <td>0.025249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>0.033177</td>\n",
       "      <td>0.035735</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.053545</td>\n",
       "      <td>0.044035</td>\n",
       "      <td>0.068070</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.084518</td>\n",
       "      <td>0.092902</td>\n",
       "      <td>0.031994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.034616</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>0.053545</td>\n",
       "      <td>0.044035</td>\n",
       "      <td>0.058230</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.080951</td>\n",
       "      <td>0.093575</td>\n",
       "      <td>0.031994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>2018-09-27</td>\n",
       "      <td>0.033151</td>\n",
       "      <td>0.034402</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.022572</td>\n",
       "      <td>0.070716</td>\n",
       "      <td>0.053842</td>\n",
       "      <td>0.065931</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.080413</td>\n",
       "      <td>0.095145</td>\n",
       "      <td>0.031994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>0.037933</td>\n",
       "      <td>0.032294</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>0.028246</td>\n",
       "      <td>0.070716</td>\n",
       "      <td>0.053842</td>\n",
       "      <td>0.065777</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.080741</td>\n",
       "      <td>0.101098</td>\n",
       "      <td>0.033343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>2018-09-29</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.040125</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.027749</td>\n",
       "      <td>0.063815</td>\n",
       "      <td>0.037246</td>\n",
       "      <td>0.067545</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.071396</td>\n",
       "      <td>0.091859</td>\n",
       "      <td>0.036042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>0.037262</td>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.024830</td>\n",
       "      <td>0.063815</td>\n",
       "      <td>0.037246</td>\n",
       "      <td>0.065313</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.075265</td>\n",
       "      <td>0.095778</td>\n",
       "      <td>0.026598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>0.032589</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.023502</td>\n",
       "      <td>0.095401</td>\n",
       "      <td>0.058261</td>\n",
       "      <td>0.046467</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.075265</td>\n",
       "      <td>0.095778</td>\n",
       "      <td>0.022551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>0.031313</td>\n",
       "      <td>0.033160</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>0.095401</td>\n",
       "      <td>0.058261</td>\n",
       "      <td>0.051106</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.075265</td>\n",
       "      <td>0.095778</td>\n",
       "      <td>0.029296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>0.030617</td>\n",
       "      <td>0.032314</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.021625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041202</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.075778</td>\n",
       "      <td>0.093278</td>\n",
       "      <td>0.025249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>2018-10-04</td>\n",
       "      <td>0.028355</td>\n",
       "      <td>0.030710</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.020930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089556</td>\n",
       "      <td>0.105089</td>\n",
       "      <td>0.021202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1738 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  btc_high  btc_close  btc_volume  btc_market_cap  \\\n",
       "0     2014-01-01  0.321552   0.331202    0.160900        0.339832   \n",
       "1     2014-01-02  0.319950   0.327366    0.162952        0.342786   \n",
       "2     2014-01-03  0.321981   0.330136    0.166809        0.344857   \n",
       "3     2014-01-04  0.324067   0.331871    0.167720        0.346238   \n",
       "4     2014-01-05  0.323588   0.333732    0.167775        0.345387   \n",
       "5     2014-01-06  0.323554   0.332510    0.182936        0.347399   \n",
       "6     2014-01-07  0.330694   0.334693    0.210234        0.349265   \n",
       "7     2014-01-08  0.327025   0.336381    0.193135        0.339428   \n",
       "8     2014-01-09  0.320677   0.326974    0.186024        0.337123   \n",
       "9     2014-01-10  0.321569   0.324462    0.198142        0.345137   \n",
       "10    2014-01-11  0.327099   0.332171    0.175117        0.350499   \n",
       "11    2014-01-12  0.329744   0.338135    0.175964        0.351023   \n",
       "12    2014-01-13  0.332180   0.338722    0.189059        0.352037   \n",
       "13    2014-01-14  0.331162   0.339394    0.273893        0.340193   \n",
       "14    2014-01-15  0.317827   0.328251    0.182282        0.334015   \n",
       "15    2014-01-16  0.313762   0.321981    0.185774        0.332536   \n",
       "16    2014-01-17  0.310528   0.320571    0.175234        0.327649   \n",
       "17    2014-01-18  0.318378   0.315907    0.163936        0.340056   \n",
       "18    2014-01-19  0.318585   0.328122    0.137212        0.341227   \n",
       "19    2014-01-20  0.319460   0.329468    0.134803        0.339737   \n",
       "20    2014-01-21  0.321189   0.327890    0.170877        0.340024   \n",
       "21    2014-01-22  0.318136   0.328128    0.176527        0.331391   \n",
       "22    2014-01-23  0.309506   0.319561    0.170374        0.329364   \n",
       "23    2014-01-24  0.311270   0.317977    0.161383        0.330123   \n",
       "24    2014-01-25  0.310065   0.318417    0.155686        0.328474   \n",
       "25    2014-01-26  0.313651   0.316925    0.153915        0.324267   \n",
       "26    2014-01-27  0.318078   0.313049    0.160760        0.336852   \n",
       "27    2014-01-28  0.319136   0.325528    0.178782        0.340478   \n",
       "28    2014-01-29  0.329191   0.328742    0.231588        0.352496   \n",
       "29    2014-01-30  0.361051   0.342390    0.243208        0.384771   \n",
       "...          ...       ...        ...         ...             ...   \n",
       "1708  2018-09-05  0.031130   0.033176    0.001135        0.023957   \n",
       "1709  2018-09-06  0.031450   0.033536    0.000635        0.023336   \n",
       "1710  2018-09-07  0.031230   0.032838    0.001763        0.021958   \n",
       "1711  2018-09-08  0.034274   0.030710    0.001945        0.026155   \n",
       "1712  2018-09-09  0.034476   0.036605    0.001232        0.024968   \n",
       "1713  2018-09-10  0.032687   0.034966    0.000900        0.022774   \n",
       "1714  2018-09-11  0.030724   0.032039    0.001345        0.023757   \n",
       "1715  2018-09-12  0.032190   0.033331    0.000535        0.024621   \n",
       "1716  2018-09-13  0.033124   0.034550    0.000654        0.025435   \n",
       "1717  2018-09-14  0.033680   0.035499    0.000677        0.025516   \n",
       "1718  2018-09-15  0.033941   0.035824    0.001040        0.025569   \n",
       "1719  2018-09-16  0.033165   0.035864    0.000902        0.024354   \n",
       "1720  2018-09-17  0.031682   0.034261    0.000637        0.023448   \n",
       "1721  2018-09-18  0.031754   0.032948    0.001517        0.024137   \n",
       "1722  2018-09-19  0.032924   0.034035    0.000683        0.025102   \n",
       "1723  2018-09-20  0.033258   0.035343    0.001059        0.024061   \n",
       "1724  2018-09-21  0.032397   0.033913    0.000754        0.024431   \n",
       "1725  2018-09-22  0.032679   0.034323    0.001792        0.025064   \n",
       "1726  2018-09-23  0.036061   0.035463    0.001542        0.027299   \n",
       "1727  2018-09-24  0.035707   0.038089    0.001758        0.025307   \n",
       "1728  2018-09-25  0.033177   0.035735    0.001217        0.024514   \n",
       "1729  2018-09-26  0.032833   0.034616    0.002397        0.024303   \n",
       "1730  2018-09-27  0.033151   0.034402    0.002992        0.022572   \n",
       "1731  2018-09-28  0.037933   0.032294    0.003291        0.028246   \n",
       "1732  2018-09-29  0.040518   0.040125    0.003470        0.027749   \n",
       "1733  2018-09-30  0.037262   0.039102    0.002938        0.024830   \n",
       "1734  2018-10-01  0.032589   0.035271    0.001474        0.023502   \n",
       "1735  2018-10-02  0.031313   0.033160    0.001466        0.022733   \n",
       "1736  2018-10-03  0.030617   0.032314    0.001495        0.021625   \n",
       "1737  2018-10-04  0.028355   0.030710    0.000824        0.020930   \n",
       "\n",
       "      bch_avg_block_size  bch_transactions  bch_mining_revenue  bch_accounts  \\\n",
       "0               0.727437          0.523625            0.214075      1.000000   \n",
       "1               0.727437          0.523625            0.205355      0.999343   \n",
       "2               0.746110          0.524973            0.238706      0.998520   \n",
       "3               0.746110          0.524973            0.245823      0.997272   \n",
       "4               0.584187          0.461143            0.227486      0.996607   \n",
       "5               0.584187          0.461143            0.236537      0.995442   \n",
       "6               0.765816          0.540268            0.213521      0.994664   \n",
       "7               0.765816          0.540268            0.204635      0.993679   \n",
       "8               0.658221          0.549203            0.248511      0.992493   \n",
       "9               0.658221          0.549203            0.223961      0.991402   \n",
       "10              0.546852          0.414158            0.229543      0.990582   \n",
       "11              0.546852          0.414158            0.238011      0.989905   \n",
       "12              0.899600          0.603796            0.219989      0.988804   \n",
       "13              0.899600          0.603796            0.225620      0.987957   \n",
       "14              0.694020          0.546398            0.225434      0.986887   \n",
       "15              0.694020          0.546398            0.203242      0.985835   \n",
       "16              0.796096          0.518017            0.195949      0.984613   \n",
       "17              0.796096          0.518017            0.204166      0.983793   \n",
       "18              0.679602          0.442193            0.207307      0.982960   \n",
       "19              0.679602          0.442193            0.194971      0.981934   \n",
       "20              0.708481          0.502521            0.217838      0.981238   \n",
       "21              0.708481          0.502521            0.241798      0.980471   \n",
       "22              0.829894          0.511287            0.202245      0.979443   \n",
       "23              0.829894          0.511287            0.192920      0.978568   \n",
       "24              0.507682          0.384416            0.234200      0.977775   \n",
       "25              0.507682          0.384416            0.217696      0.976860   \n",
       "26              0.778555          0.493607            0.224710      0.976044   \n",
       "27              0.778555          0.493607            0.221641      0.975174   \n",
       "28              0.895785          0.534508            0.215921      0.974383   \n",
       "29              0.895785          0.534508            0.211233      0.973446   \n",
       "...                  ...               ...                 ...           ...   \n",
       "1708            0.083732          0.052955            0.049419      0.006758   \n",
       "1709            0.083732          0.052955            0.048978      0.006758   \n",
       "1710            0.089990          0.050244            0.051765      0.006021   \n",
       "1711            0.089990          0.050244            0.048066      0.006021   \n",
       "1712            0.075716          0.036229            0.051478      0.006021   \n",
       "1713            0.075716          0.036229            0.047993      0.005254   \n",
       "1714            0.081302          0.057495            0.062186      0.005254   \n",
       "1715            0.081302          0.057495            0.057465      0.005254   \n",
       "1716            0.110552          0.078338            0.062060      0.004489   \n",
       "1717            0.110552          0.078338            0.055929      0.004489   \n",
       "1718            0.035210          0.020202            0.060983      0.004489   \n",
       "1719            0.035210          0.020202            0.061475      0.003836   \n",
       "1720            0.092124          0.044119            0.050712      0.003836   \n",
       "1721            0.092124          0.044119            0.046309      0.003836   \n",
       "1722            0.075054          0.049321            0.067627      0.003153   \n",
       "1723            0.075054          0.049321            0.057637      0.003153   \n",
       "1724            0.032692          0.034626            0.048694      0.003153   \n",
       "1725            0.032692          0.034626            0.066565      0.002518   \n",
       "1726            0.039344          0.039079            0.065465      0.002518   \n",
       "1727            0.039344          0.039079            0.076389      0.002518   \n",
       "1728            0.053545          0.044035            0.068070      0.001869   \n",
       "1729            0.053545          0.044035            0.058230      0.001869   \n",
       "1730            0.070716          0.053842            0.065931      0.001869   \n",
       "1731            0.070716          0.053842            0.065777      0.001117   \n",
       "1732            0.063815          0.037246            0.067545      0.001117   \n",
       "1733            0.063815          0.037246            0.065313      0.001117   \n",
       "1734            0.095401          0.058261            0.046467      0.000499   \n",
       "1735            0.095401          0.058261            0.051106      0.000499   \n",
       "1736            0.000000          0.000000            0.041202      0.000499   \n",
       "1737            0.000000          0.000000            0.059562      0.000000   \n",
       "\n",
       "      sp_close  dj_close  google_trends_bitcoin  \n",
       "0     0.975489  0.982462               0.041897  \n",
       "1     0.995592  1.000000               0.043465  \n",
       "2     0.993843  0.995247               0.041897  \n",
       "3     0.994819  0.984533               0.038763  \n",
       "4     0.985894  0.967694               0.037195  \n",
       "5     0.985894  0.967694               0.037195  \n",
       "6     0.985894  0.967694               0.046600  \n",
       "7     0.985911  0.966090               0.045032  \n",
       "8     0.979156  0.961319               0.046600  \n",
       "9     0.987223  0.970654               0.048167  \n",
       "10    0.990428  0.976750               0.045032  \n",
       "11    0.999092  0.992590               0.040330  \n",
       "12    0.999092  0.992590               0.043465  \n",
       "13    0.999092  0.992590               0.052869  \n",
       "14    1.000000  0.985037               0.046600  \n",
       "15    0.980822  0.963107               0.046600  \n",
       "16    0.977760  0.949245               0.045032  \n",
       "17    0.964714  0.933109               0.048167  \n",
       "18    0.978324  0.941188               0.040330  \n",
       "19    0.978324  0.941188               0.041897  \n",
       "20    0.978324  0.941188               0.049735  \n",
       "21    0.977651  0.940431               0.054437  \n",
       "22    0.964815  0.927592               0.057572  \n",
       "23    0.963949  0.925161               0.051302  \n",
       "24    0.954898  0.915210               0.052869  \n",
       "25    0.950314  0.920401               0.046600  \n",
       "26    0.950314  0.920401               0.046600  \n",
       "27    0.950314  0.920401               0.057572  \n",
       "28    0.955672  0.927326               0.076381  \n",
       "29    0.964546  0.925504               0.068544  \n",
       "...        ...       ...                    ...  \n",
       "1708  0.043992  0.041535               0.025780  \n",
       "1709  0.027177  0.031949               0.030096  \n",
       "1710  0.042570  0.048514               0.041965  \n",
       "1711  0.033368  0.040599               0.036570  \n",
       "1712  0.040711  0.044198               0.024701  \n",
       "1713  0.040711  0.044198               0.023622  \n",
       "1714  0.040711  0.044198               0.031175  \n",
       "1715  0.072818  0.071978               0.031175  \n",
       "1716  0.086612  0.087341               0.035491  \n",
       "1717  0.085721  0.090928               0.032254  \n",
       "1718  0.081431  0.094780               0.030096  \n",
       "1719  0.081431  0.094780               0.023622  \n",
       "1720  0.081431  0.094780               0.023622  \n",
       "1721  0.081431  0.094780               0.034412  \n",
       "1722  0.087479  0.091153               0.034412  \n",
       "1723  0.089573  0.096821               0.027938  \n",
       "1724  0.081582  0.087386               0.027938  \n",
       "1725  0.065029  0.077267               0.026598  \n",
       "1726  0.084518  0.092902               0.026598  \n",
       "1727  0.084518  0.092902               0.025249  \n",
       "1728  0.084518  0.092902               0.031994  \n",
       "1729  0.080951  0.093575               0.031994  \n",
       "1730  0.080413  0.095145               0.031994  \n",
       "1731  0.080741  0.101098               0.033343  \n",
       "1732  0.071396  0.091859               0.036042  \n",
       "1733  0.075265  0.095778               0.026598  \n",
       "1734  0.075265  0.095778               0.022551  \n",
       "1735  0.075265  0.095778               0.029296  \n",
       "1736  0.075778  0.093278               0.025249  \n",
       "1737  0.089556  0.105089               0.021202  \n",
       "\n",
       "[1738 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_csv('model_data.csv').iloc[:,1:]\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_info.head()\n",
    "market_info.isnull().values.any()\n",
    "market_info.to_csv('market_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import minmax_scale, Imputer\n",
    "\n",
    "# # model_data.fillna(model_data.mean(),inplace=True)\n",
    "\n",
    "# mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "\n",
    "# btc_without_date = model_data.loc[:, model_data.columns != 'Date']\n",
    "\n",
    "# # Train the imputor on the df dataset\n",
    "# mean_imputer = mean_imputer.fit(btc_without_date)\n",
    "\n",
    "# # Apply the imputer to the df dataset\n",
    "# imputed_df = mean_imputer.transform(btc_without_date)\n",
    "\n",
    "# # Get back columns\n",
    "# imputed_df = pd.DataFrame(imputed_df, columns = btc_without_date.columns)\n",
    "\n",
    "# imputed_df[['bt_Close','bt_Volume', 'sp_close', 'dj_close', 'bt_google_trends_bitcoin','bt_avg_block_size', \\\n",
    "#                                     'bt_mining_revenue', 'bt_transactions', 'bt_Accounts']] = \\\n",
    "#                 minmax_scale(imputed_df[['bt_Close', 'sp_close', 'dj_close', 'bt_Volume', \\\n",
    "#                                         'bt_mining_revenue', 'bt_google_trends_bitcoin','bt_avg_block_size', 'bt_transactions','bt_Accounts']])\n",
    "\n",
    "# # imputed_df[['bt_Close','bt_Volume', 'bt_google_trends_bitcoin']] = \\\n",
    "# #         minmax_scale(imputed_df[['bt_Close','bt_Volume', 'bt_google_trends_bitcoin']])\n",
    "\n",
    "# # imputed_df[['bt_Close','bt_Volume']] = \\\n",
    "# #         minmax_scale(imputed_df[['bt_Close','bt_Volume']])\n",
    "\n",
    "# # Re add date column\n",
    "# imputed_df['Date'] = model_data['Date']\n",
    "\n",
    "# # Order by date\n",
    "# imputed_df[\"Date\"] = imputed_df[\"Date\"].values[::-1]\n",
    "\n",
    "# model_data=imputed_df\n",
    "# # model_data.to_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>sp_close</th>\n",
       "      <th>dj_close</th>\n",
       "      <th>btc_Open</th>\n",
       "      <th>btc_High</th>\n",
       "      <th>btc_Close</th>\n",
       "      <th>btc_Volume</th>\n",
       "      <th>btc_Market Cap</th>\n",
       "      <th>btc_google_trends_bitcoin</th>\n",
       "      <th>btc_avg_block_size</th>\n",
       "      <th>btc_transactions</th>\n",
       "      <th>btc_Accounts</th>\n",
       "      <th>btc_mining_revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0.202753</td>\n",
       "      <td>0.158649</td>\n",
       "      <td>0.035372</td>\n",
       "      <td>0.035014</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>0.031834</td>\n",
       "      <td>0.077695</td>\n",
       "      <td>0.065017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.147451</td>\n",
       "      <td>0.036323</td>\n",
       "      <td>0.037261</td>\n",
       "      <td>0.037777</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.026582</td>\n",
       "      <td>0.027865</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>0.031834</td>\n",
       "      <td>0.096234</td>\n",
       "      <td>0.046764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.149821</td>\n",
       "      <td>0.037839</td>\n",
       "      <td>0.037952</td>\n",
       "      <td>0.038617</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.027684</td>\n",
       "      <td>0.031213</td>\n",
       "      <td>0.119025</td>\n",
       "      <td>0.088241</td>\n",
       "      <td>0.117316</td>\n",
       "      <td>0.056611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.149821</td>\n",
       "      <td>0.038891</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.040717</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>0.025633</td>\n",
       "      <td>0.119025</td>\n",
       "      <td>0.088241</td>\n",
       "      <td>0.096473</td>\n",
       "      <td>0.051999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.149821</td>\n",
       "      <td>0.040709</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.044526</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.029771</td>\n",
       "      <td>0.030097</td>\n",
       "      <td>0.088264</td>\n",
       "      <td>0.067895</td>\n",
       "      <td>0.099443</td>\n",
       "      <td>0.070736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  sp_close  dj_close  btc_Open  btc_High  btc_Close  \\\n",
       "1726  2014-01-01  0.202753  0.158649  0.035372  0.035014   0.036182   \n",
       "1725  2014-01-02  0.190688  0.147451  0.036323  0.037261   0.037777   \n",
       "1724  2014-01-03  0.190239  0.149821  0.037839  0.037952   0.038617   \n",
       "1723  2014-01-04  0.190239  0.149821  0.038891  0.039219   0.040717   \n",
       "1722  2014-01-05  0.190239  0.149821  0.040709  0.043860   0.044526   \n",
       "\n",
       "      btc_Volume  btc_Market Cap  btc_google_trends_bitcoin  \\\n",
       "1726    0.000943        0.025890                   0.024518   \n",
       "1725    0.001614        0.026582                   0.027865   \n",
       "1724    0.001586        0.027684                   0.031213   \n",
       "1723    0.001594        0.028449                   0.025633   \n",
       "1722    0.003058        0.029771                   0.030097   \n",
       "\n",
       "      btc_avg_block_size  btc_transactions  btc_Accounts  btc_mining_revenue  \n",
       "1726            0.026115          0.031834      0.077695            0.065017  \n",
       "1725            0.026115          0.031834      0.096234            0.046764  \n",
       "1724            0.119025          0.088241      0.117316            0.056611  \n",
       "1723            0.119025          0.088241      0.096473            0.051999  \n",
       "1722            0.088264          0.067895      0.099443            0.070736  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date='2018-06-01'\n",
    "# we don't need the date columns anymore\n",
    "training_set, test_set = model_data[model_data['Date']<split_date], model_data[model_data['Date']>=split_date]\n",
    "training_set = training_set.drop('Date', 1)\n",
    "test_set = test_set.drop('Date', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 65\n",
    "\n",
    "LSTM_training_inputs = []\n",
    "for i in range(len(training_set)-window_len):\n",
    "    temp_set = training_set[i:(i+window_len)].copy()\n",
    "    LSTM_training_inputs.append(temp_set)   \n",
    "    \n",
    "# LSTM_training_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "LSTM_test_inputs = []\n",
    "for i in range(len(test_set)-window_len):\n",
    "    temp_set = test_set[i:(i+window_len)].copy()\n",
    "    LSTM_test_inputs.append(temp_set)\n",
    "    \n",
    "#     for col in norm_cols:\n",
    "#         temp_set.loc[:, col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
    "#     LSTM_test_inputs.append(temp_set)\n",
    "\n",
    "# LSTM_test_inputs = []\n",
    "# for i in range(len(test_set)-window_len):\n",
    "#     temp_set = test_set[i:(i+window_len)].copy()\n",
    "#     LSTM_test_inputs.append(temp_set)\n",
    "# LSTM_test_outputs = test_set['bt_Close'][window_len:].values\n",
    "# LSTM_test_outputs = test_set['bt_Close'][window_len:].values-1\n",
    "\n",
    "# print(LSTM_test_inputs[0])\n",
    "LSTM_test_outputs = test_set['btc_Close'][window_len:].values\n",
    "print(len(LSTM_test_outputs)) # predicting 45 points in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table represents an example of our LSTM model input (we'll actually have hundreds of similar tables). We've normalised some columns so that their values are equal to 0 in the first time point, so we're aiming to predict changes in price relative to this timepoint. We're now ready to build the LSTM model. This is actually quite straightforward with Keras, you simply stack componenets on top of each other (better explained [here](https://dashee87.github.io/data%20science/deep%20learning/python/another-keras-tutorial-for-neural-network-beginners/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_training_inputs = [np.array(LSTM_training_input) for LSTM_training_input in LSTM_training_inputs]\n",
    "LSTM_training_inputs = np.array(LSTM_training_inputs)\n",
    "\n",
    "LSTM_test_inputs = [np.array(LSTM_test_inputs) for LSTM_test_inputs in LSTM_test_inputs]\n",
    "LSTM_test_inputs = np.array(LSTM_test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the relevant Keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def build_model(inputs, output_size, neurons, activ_func=\"tanh\",\n",
    "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def gru_model(inputs, output_size, neurons, activ_func=\"tanh\",\n",
    "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(0.15))  # Dropout overfitting\n",
    "\n",
    "    # model.add(GRU(layers[2],activation='tanh', return_sequences=True))\n",
    "    # model.add(Dropout(0.2))  # Dropout overfitting\n",
    "\n",
    "    model.add(GRU(neurons, input_shape=(inputs.shape[1], inputs.shape[2]), \n",
    "                  activation='tanh', return_sequences=False))\n",
    "    model.add(Dropout(0.15))  # Dropout overfitting\n",
    "\n",
    "    model.add(Dense(output_dim=layers[3]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # model.compile(loss=\"mse\", optimizer=sgd)\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\") # Nadam rmsprop\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model \n",
    "\n",
    "\n",
    "\n",
    "def denser_model(inputs, output_size, neurons, activ_func='tanh', \n",
    "                 dropout=0.3, loss='mae', optimizer='adam'):\n",
    "    \"\"\"\n",
    "    inputs: input data as numpy array\n",
    "    output_size: number of predictions per input sample\n",
    "    neurons: number of neurons/ units in the LSTM layer\n",
    "    active_func: Activation function to be used in LSTM layers and Dense layer\n",
    "    dropout: dropout ration, default is 0.25\n",
    "    loss: loss function for calculating the gradient\n",
    "    optimizer: type of optimizer to backpropagate the gradient\n",
    "    This function will build 3 layered RNN model with LSTM cells with dropouts after each LSTM layer \n",
    "    and finally a dense layer to produce the output using keras' sequential model.\n",
    "    Return: Keras sequential model and model summary\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, return_sequences=True, input_shape=(inputs.shape[1], inputs.shape[2]), activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(neurons, return_sequences=True, activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(neurons, activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00013822, -0.06106792,  0.10776572, ...,  0.10898142,\n",
      "        -0.09600256,  0.03442807],\n",
      "       [ 0.09821731,  0.00263345, -0.05255553, ...,  0.00569086,\n",
      "        -0.02730787, -0.04569017],\n",
      "       [ 0.0780047 , -0.0460289 ,  0.09915353, ...,  0.0528841 ,\n",
      "        -0.0464635 ,  0.08980535],\n",
      "       ...,\n",
      "       [ 0.09350048, -0.09420775,  0.01824959, ...,  0.10316053,\n",
      "        -0.00692932,  0.04098645],\n",
      "       [ 0.0526219 ,  0.03779012,  0.03271791, ..., -0.00932365,\n",
      "        -0.03446206,  0.10858494],\n",
      "       [-0.10022227, -0.03279163, -0.11224488, ..., -0.11940891,\n",
      "        -0.07235562, -0.04712335]], dtype=float32), array([[-0.13244021,  0.04335991,  0.06624217, ...,  0.04899031,\n",
      "        -0.08540058, -0.05648996],\n",
      "       [ 0.05374488,  0.03159233, -0.01711521, ...,  0.02125785,\n",
      "         0.01233767,  0.002683  ],\n",
      "       [ 0.0327259 , -0.03872018, -0.01054513, ..., -0.04347579,\n",
      "        -0.05101265, -0.01117337],\n",
      "       ...,\n",
      "       [-0.02732293, -0.00524141, -0.05093112, ..., -0.05364712,\n",
      "        -0.00802958, -0.07677154],\n",
      "       [-0.0213811 , -0.01298171,  0.00230804, ..., -0.05751638,\n",
      "        -0.06602672,  0.00464818],\n",
      "       [ 0.00315823, -0.04692919, -0.07545283, ..., -0.00807184,\n",
      "        -0.03105382,  0.07356793]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.16305791, -0.04133168,  0.15143555, ...,  0.06981599,\n",
      "         0.16727254,  0.02403536],\n",
      "       [ 0.14016768,  0.10184184, -0.06096455, ..., -0.11521683,\n",
      "         0.13345915, -0.00116158],\n",
      "       [-0.10390995,  0.0305872 ,  0.13629678, ...,  0.17671561,\n",
      "        -0.10222167,  0.02815683],\n",
      "       ...,\n",
      "       [-0.00110756, -0.16348115,  0.08669472, ...,  0.07791671,\n",
      "         0.02832073,  0.10604557],\n",
      "       [ 0.0330395 , -0.1018218 ,  0.01745491, ...,  0.10113806,\n",
      "        -0.18638518, -0.10390478],\n",
      "       [ 0.14762583, -0.16207343,  0.07863143, ..., -0.17940734,\n",
      "         0.09993434,  0.00451183]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.0894\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.0509\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.0393\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.0340\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.0320\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.0298\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.0284\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.0256\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.0249\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.0235\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.0226\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.0246\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.0216\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.0222\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.0217\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.0217\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.0218\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.0200\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.0197\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.0196\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.0196\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.0191\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.0187\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.0191\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.0191\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.0183\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.0185\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.0195\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.0187\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.0177\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.0177\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.0171\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.0173\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.0172\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.0174\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.0166\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.0165\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.0161\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.0158\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.0160\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.0156\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.0161\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.0161\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.0161\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.0159\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.0160\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.0153\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.0155\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.0154\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.0154\n"
     ]
    }
   ],
   "source": [
    "# random seed for reproducibility\n",
    "np.random.seed(202)\n",
    "\n",
    "pred_range=60\n",
    "\n",
    "# initialise model architecture\n",
    "bt_model = build_model(LSTM_training_inputs, output_size=pred_range, neurons = 100)\n",
    "# bt_model = denser_model(LSTM_training_inputs, output_size=pred_range, neurons = 100)\n",
    "\n",
    "# model output is next 5 prices normalised to 10th previous closing price\n",
    "LSTM_training_outputs = []\n",
    "\n",
    "for i in range(window_len, len(training_set['btc_Close'])-pred_range):\n",
    "    LSTM_training_outputs.append(training_set['btc_Close'][i:i+pred_range].values)\n",
    "    \n",
    "LSTM_training_outputs = np.array(LSTM_training_outputs)\n",
    "print(bt_model.get_weights())\n",
    "# train model on data\n",
    "bt_history = bt_model.fit(LSTM_training_inputs[:-pred_range], LSTM_training_outputs, \n",
    "                            epochs=50, batch_size=120, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz;\n",
    "from keras.utils.vis_utils import plot_model\n",
    "graph = plot_model(bt_model, to_file=\"my_model.png\", show_shapes=True)\n",
    "\n",
    "#Build your model here\n",
    "# ann_viz(bt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([0,0,1,1])\n",
    "x2 = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# line, = ax.plot(x1, x2, lw=2)\n",
    "\n",
    "# ax.annotate('local max', xy=(2, 1), xytext=(3, 1.5),\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "#             )\n",
    "\n",
    "ax.set_ylim(0,1.75)\n",
    "ax.set_xlim(0,1.75)\n",
    "\n",
    "# plt.scatter([0,1],[1,0], s=500,marker=\"o\", c=\"r\")\n",
    "line = plt.scatter(1.00,1.00, s=200,marker=\"o\", c=\"b\")\n",
    "line2=plt.scatter(0.00,0.00, s=200,marker=\"o\", c=\"b\")\n",
    "line3 = plt.scatter(0,1.00, s=400,marker=\"x\", c=\"r\")\n",
    "line4=plt.scatter(1.00,0.00, s=400,marker=\"x\", c=\"r\")\n",
    "line3.set_clip_on(False)\n",
    "line2.set_clip_on(False)\n",
    "line4.set_clip_on(False)\n",
    "x1, y1 = [-0.2,0.8], [0.8, -0.2]\n",
    "\n",
    "x2, y2 = [-0.2,1.5], [1.5, -0.2]\n",
    "\n",
    "line5=plt.plot(x1, y1, x2, y2, clip_on = False,color=\"green\")\n",
    "plt.title(\"XOR: Klasifikimi i pamundur nga nje vije\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 6, 6\n",
    "\n",
    "# fig.clear()\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "ax1.plot(bt_history.epoch, bt_history.history['loss'])\n",
    "ax1.set_title('Training Error')\n",
    "\n",
    "if bt_model.loss == 'mae':\n",
    "    ax1.set_ylabel('Mean Absolute Error (MAE)',fontsize=12)\n",
    "# just in case you decided to change the model loss calculation\n",
    "else:\n",
    "    ax1.set_ylabel('Model Loss',fontsize=12)\n",
    "ax1.set_xlabel('Nr. of Epochs',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 35, 12)\n"
     ]
    }
   ],
   "source": [
    "# params = 4 * ((len(LSTM_training_inputs[:-pred_range]) + 1) * len(LSTM_training_outputs) +  len(LSTM_training_outputs)^2)\n",
    "\n",
    "# # 4 * (4097 * 256 + 256^2) = 4457472\n",
    "# print(params)\n",
    "\n",
    "# 4 * 100 * (len(LSTM_training_inputs[:-pred_range]) + 1 + len(LSTM_training_outputs))\n",
    "\n",
    "# LSTM_training_inputs, output_size=pred_range, neurons = 100\n",
    "\n",
    "# model.add(LSTM(units=256, input_dim=4096, input_length=16))\n",
    "\n",
    "# [(256 + 4096 + 1) * 256] * 4 = 4457472\n",
    "# units=100\n",
    "# input_dim=len(LSTM_training_inputs[:-pred_range])\n",
    "# input_length=12\n",
    "              \n",
    "# p = ((units+input_dim+1)*units)*4\n",
    "              \n",
    "\n",
    "# The entities W , U and V are shared by all steps of the RNN and these are the only parameters in the model described in the figure. Hence number of parameters to be learnt while training = dim(W)+dim(V)+dim(U).\n",
    "\n",
    "# Based on data in the question this = n2+kn+nm.\n",
    "\n",
    "# where,\n",
    "\n",
    "# n - dimension of hidden layer - 100 \n",
    "# k - dimension of output layer - 30 \n",
    "# m - dimension of input layer - 1611\n",
    "\n",
    "\n",
    "d=100^2 + 30*100 + 100*1611\n",
    "d\n",
    "print(LSTM_training_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Training History: 0.022062765859034873\n",
      "Scikit: 0.012192275871242015\n",
      "Mean from Prediction: 0.0122\n",
      "dict_keys(['loss'])\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "# From History though: \n",
    "print(\"From Training History: \" + str(np.mean(bt_history.history['loss'])) )\n",
    "\n",
    "a=bt_model.predict(LSTM_training_inputs[:-pred_range])\n",
    "\n",
    "print( \"Scikit: \" + str(sklearn.metrics.mean_absolute_error(LSTM_training_outputs, bt_model.predict(LSTM_training_inputs[:-pred_range]))))\n",
    "\n",
    "print('Mean from Prediction: %.4f'%np.mean(np.abs((bt_model.predict(LSTM_training_inputs[:-pred_range]))-\\\n",
    "            (LSTM_training_outputs))))\n",
    "\n",
    "# print('MAE: %.4f'%np.mean(np.abs((bt_model.predict(LSTM_test_inputs[:-pred_range]))-\\\n",
    "#             (LSTM_training_outputs))))\n",
    "print(bt_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kejsistruga/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import sklearn.metrics\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "mae_test_error =sklearn.metrics.mean_absolute_error(LSTM_training_outputs, bt_model.predict(LSTM_training_inputs[:-pred_range]))\n",
    "\n",
    "# Create a trace\n",
    "real_price = go.Scatter(\n",
    "    x = model_data[model_data['Date']< split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "    y = training_set['btc_Close'][window_len:]+1,\n",
    "    name = \"Real Price\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgba(152, 0, 0, .8)'\n",
    "    )\n",
    ")\n",
    "\n",
    "predicted_price = go.Scatter(\n",
    "    x = model_data[model_data['Date']< split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "    y = ((np.transpose(bt_model.predict(LSTM_training_inputs))+1))[0], \n",
    "    name = \"Predicted Price\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = \"#82E0AA\"\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Training Set Prediction, MAE: %.4f'%+ mae_test_error,\n",
    "              yaxis = dict(title = 'Bitcoin Price (USD)'),\n",
    "            )\n",
    "\n",
    "data = [real_price,predicted_price]\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='bitcoin-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kejsistruga/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "mae_test_error =np.mean(np.abs((np.transpose(bt_model.predict(LSTM_test_inputs)))-\\\n",
    "            (test_set['btc_Close'].values[window_len:])))\n",
    "# Create a trace\n",
    "real_price = go.Scatter(\n",
    "    x = model_data[model_data['Date']>= split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "    y = test_set['btc_Close'][window_len:],\n",
    "    name = \"Real Price\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgba(152, 0, 0, .8)'\n",
    "    )\n",
    ")\n",
    "\n",
    "predicted_price = go.Scatter(\n",
    "    x = model_data[model_data['Date']>= split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "    y = ((np.transpose(bt_model.predict(LSTM_test_inputs)))[0]), \n",
    "    name = \"Predicted Price\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = \"#82E0AA\"\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Test Set Prediction (Batch Size=200), MAE: %.4f'%+ mae_test_error,\n",
    "              yaxis = dict(title = 'Bitcoin Price (USD)')\n",
    "            )\n",
    "\n",
    "data = [real_price,predicted_price]\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='bitcoin-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_1 to have shape (60,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9f69658fc449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_test_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpred_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_1 to have shape (60,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    \n",
    "    EVALUATING ON TEST SET; MAE: 0.0301\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_outputs=[]\n",
    "\n",
    "for i in range(window_len, len(test_set['btc_Close'])-pred_range):\n",
    "    test_outputs.append(test_set['btc_Close'][i:i+pred_range].values)\n",
    "\n",
    "a = len(test_set['btc_Close'])-pred_range\n",
    "print(a)\n",
    "test_outputs = np.array(test_outputs)\n",
    "    \n",
    "print(bt_model.evaluate(LSTM_test_inputs[:-pred_range], test_outputs, batch_size=200))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # loss: 0.030180492639249445\n",
    "# # loss with google: 0.03131183715485244\n",
    "\n",
    "# \"\"\"\n",
    "    \n",
    "#     EVALUATING ON TRAINING SET; MAE: 0.0301\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "print(bt_model.evaluate(LSTM_training_inputs[:-pred_range], LSTM_training_outputs, batch_size=200))  \n",
    "\n",
    "# # loss: 0.004505583531820338\n",
    "# # loss with google: 0.005251145405104876 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               45200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 60)                0         \n",
      "=================================================================\n",
      "Total params: 51,260\n",
      "Trainable params: 51,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# bt_model\n",
    "bt_model.summary()\n",
    "bt_model.save('bt_model_dense.h5')\n",
    "# bt_model.layers \n",
    "# # List of input tensors:\n",
    "# bt_model.inputs\n",
    "# bt_model.outputs\n",
    "bt_model.get_weights()\n",
    "bt_model.save_weights('bt_model_weights') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(bt_model, to_file='model.png')\n",
    "bt_model = build_model(LSTM_training_inputs, output_size=pred_range, neurons = 100)\n",
    "\n",
    "def build_model(inputs, output_size, neurons, activ_func=\"tanh\",\n",
    "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_dim=4096, input_length=16))\n",
    "   \n",
    "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# inputs, output_size, neurons,\n",
    "\n",
    "# model.add(LSTM(256, input_dim=4096, input_length=16))\n",
    "\n",
    "# model.add(LSTM(256, input_dim=4096, input_length=16))\n",
    "# model.summary()\n",
    "# 4 * (4097 * 256 + 256^2) = 4457472\n",
    "# 4 * ((input_dim+1) + neurons + neurons^2)\n",
    "\n",
    "4 * ((1) * 100 + 100^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'size_of_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-84e82eee2c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLSTM_training_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_of_input\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize_of_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize_of_output\u001b[0m\u001b[0;34m^\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'size_of_input' is not defined"
     ]
    }
   ],
   "source": [
    "LSTM_training_inputs.shape[1] # 35\n",
    "LSTM_training_inputs.shape[2] # 12\n",
    "\n",
    "params = 4 * ((size_of_input + 1) * size_of_output + size_of_output^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36 + 100 + 100^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "238*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144808"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*((360 + 1) * 100 + 100^2)\n",
    "# 45200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4480"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 * (4097 * 256 + 256^2) = 4457472\n",
    "# 4097 input_dim\n",
    "128*35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7236"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*(len(LSTM_training_inputs) + 1 * 100 + 100 ^ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420776"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4(nm+n2+n)\n",
    "# input vectors of size m\n",
    "# giving output vectors of size n\n",
    "4*(1050*100 + 100^2 + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "200ff45abb3b47f3ade09c6ce91695fb": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
